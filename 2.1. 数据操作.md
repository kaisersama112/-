## 2.1 数据操作
为了能够完成各种数据操作，我们需要某种方法来存储和操作数据。 通常，我们需要做两件重要的事：
    （1）获取数据；
    （2）将数据读入计算机后对其进行处理。 如果没有某种方法来存储数据，那么获取数据是没有意义的。
    首先，我们介绍n维数组，也称为张量（tensor）。 使用过Python中NumPy计算包的读者会对本部分很熟悉。 无论使用哪个深度学习框架，它的张量类（在MXNet中为ndarray， 在PyTorch和TensorFlow中为Tensor）都与Numpy的ndarray类似。 但深度学习框架又比Numpy的ndarray多一些重要功能： 首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算； 其次，张量类支持自动微分。 这些功能使得张量类更适合深度学习。 如果没有特殊说明，本书中所说的张量均指的是张量类的实例。


```python
import torch
```

张量表示一个由数值组成的数组，这个数组可能有多个维度，具有一个轴的张量对于数学上的向量（vector）,具有两个轴的张量对应数学上的矩阵（matrix）;具有两个轴以上的张量没有特殊的数学名称。
创建一个行向量x 这个行向量包含以0开始的前12个整数，他们默认创建为整数，也可以指定创建类型为浮点数，张量中的每个值都称为张量的元素（element）。例如：张量 x 中有 12 个元素。除非额外指定，新的张量将存储在内存中，并采用基于CPU的计算。



```python
x = torch.arange(12)
print(x)

y = torch.arange(0, 12, dtype=float)
print(y)
z = torch.arange(0, 1, 0.1)

print(z)
```

    tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
    tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],
           dtype=torch.float64)
    tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,
            0.9000])


使用shape属性来访问张量（沿每个轴的长度）的形状 。



```python
print(x.shape)
print(y.shape)
```

    torch.Size([12])
    torch.Size([12])

查看张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）。 因为这里在处理的是一个向量，所以它的shape与它的size相同。

```python
print(x.numel())
print(y.numel())
```

    12
    12

如果想改变一个张量的形状而不改变元素数量的元素值，可以调用reshape 函数 

```python
x = x.reshape(3, 4)
y = y.reshape(4, 3)
print(x)
print(y)
```

    tensor([[ 0,  1,  2,  3],
            [ 4,  5,  6,  7],
            [ 8,  9, 10, 11]])
    tensor([[ 0.,  1.,  2.],
            [ 3.,  4.,  5.],
            [ 6.,  7.,  8.],
            [ 9., 10., 11.]], dtype=torch.float64)



```python
print(x.shape)
print(y.shape)
```

    torch.Size([3, 4])
    torch.Size([4, 3])


我们不需要通过手动指定每个维度来改变形状。 也就是说，如果我们的目标形状是（高度,宽度）， 那么在知道宽度后，高度会被自动计算得出，不必我们自己做除法。 在上面的例子中，为了获得一个3行的矩阵，我们手动指定了它有3行和4列。 幸运的是，我们可以通过-1来调用此自动计算出维度的功能。 即我们可以用x.reshape(-1,4)或x.reshape(3,-1)来取代x.reshape(3,4)。


```python
x = torch.arange(0, 12)
x = x.reshape(-1, 4)
print(x)
print(x.shape)
```

    tensor([[ 0,  1,  2,  3],
            [ 4,  5,  6,  7],
            [ 8,  9, 10, 11]])
    torch.Size([3, 4])


创建全0，全1，其他常量，或者从特定分布中随机采样的数字来初始化矩阵，我们可以创建一个形状为（2，3，4）的张量，其中所有的元素都设置为0


```python
x = torch.zeros((2, 3, 4))
print(x)
print(x.shape)
y = torch.ones((2, 3, 4))
print(y)
print(y.shape)
```

    tensor([[[0., 0., 0., 0.],
             [0., 0., 0., 0.],
             [0., 0., 0., 0.]],
    
            [[0., 0., 0., 0.],
             [0., 0., 0., 0.],
             [0., 0., 0., 0.]]])
    torch.Size([2, 3, 4])
    tensor([[[1., 1., 1., 1.],
             [1., 1., 1., 1.],
             [1., 1., 1., 1.]],
    
            [[1., 1., 1., 1.],
             [1., 1., 1., 1.],
             [1., 1., 1., 1.]]])
    torch.Size([2, 3, 4])

有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。 例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。 以下代码创建一个形状为（3,4）的张量。 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。

```python
z = torch.randn((3, 4))
print(z)
print(z.shape)
```

    tensor([[ 0.6378, -0.3172, -1.4313,  1.2711],
            [-0.7190, -0.8561,  0.7441,  0.5401],
            [ 2.2458, -0.0089,  1.6704,  0.0695]])
    torch.Size([3, 4])


我们还可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。 在这里，最外层的列表对应于轴0，内层的列表对应于轴1。


```python
array = [[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]
x = torch.tensor(array)
print(x)
print(x.shape)
```

    tensor([[2, 1, 4, 3],
            [1, 2, 3, 4],
            [4, 3, 2, 1]])
    torch.Size([3, 4])


## 2.1.2. 运算符
![image-20231204144835021](./assets/Snipaste_2023-12-04_14-49-24.png)




```python
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
print(x + y)
print(x - y)
print(x * y)
print(x / y)
print(x ** y)  # **运算符是求幂运算
```

    tensor([ 3.,  4.,  6., 10.])
    tensor([-1.,  0.,  2.,  6.])
    tensor([ 2.,  4.,  8., 16.])
    tensor([0.5000, 1.0000, 2.0000, 4.0000])
    tensor([ 1.,  4., 16., 64.])


“按元素”方式可以应用更多的计算，包括像求幂这样的一元运算符。


```python
print(torch.exp(x))
```

    tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])




![image-2](./assets/Snipaste_2023-12-04_14-54-56.png)


```python
x = torch.arange(12, dtype=torch.float32).reshape(3, -1)
y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
print(x)
print(y)
# 纵轴链接
z1 = torch.cat((x, y), dim=0)
# 横轴链接 
z2 = torch.cat((x, y), dim=1)
print(z1)
print(z2)
print(z1.numel())
print(z2.numel())

```

    tensor([[ 0.,  1.,  2.,  3.],
            [ 4.,  5.,  6.,  7.],
            [ 8.,  9., 10., 11.]])
    tensor([[2., 1., 4., 3.],
            [1., 2., 3., 4.],
            [4., 3., 2., 1.]])
    tensor([[ 0.,  1.,  2.,  3.],
            [ 4.,  5.,  6.,  7.],
            [ 8.,  9., 10., 11.],
            [ 2.,  1.,  4.,  3.],
            [ 1.,  2.,  3.,  4.],
            [ 4.,  3.,  2.,  1.]])
    tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
            [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
            [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])
    24
    24


有时，我们想通过逻辑运算符构建二元张量。 以X == Y为例： 对于每个位置，如果X和Y在该位置相等，则新张量中相应项的值为1。 这意味着逻辑语句X == Y在该位置处为真，否则该位置为0。


```python
print(x == y)
```

    tensor([[False,  True, False,  True],
            [False, False, False, False],
            [False, False, False, False]])


对张量中的所有元素进行求和，会产生一个单元素张量。


```python
print(x.sum())
```

    tensor(66.)


## 2.1.3. 广播机制
在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：

通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；

对生成的数组执行按元素操作。

在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：


```python
a = torch.arange(3).reshape(3, 1)
b = torch.arange(2).reshape(1, 2)
print(a)
print(b)
```

    tensor([[0],
            [1],
            [2]])
    tensor([[0, 1]])


由于a和b分别是3x1和1x2矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵广播为一个更大的3x2矩阵，如下所示：矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。


```python
print(a + b)
```

    tensor([[0, 1],
            [1, 2],
            [2, 3]])


## 2.1.4. 索引和切片


```python
print(x[-1])
print(x[1:3])
```

    tensor([ 8.,  9., 10., 11.])
    tensor([[ 4.,  5.,  6.,  7.],
            [ 8.,  9., 10., 11.]])



```python
print(x)
x[1, 2] = 9
print(x)
```

    tensor([[ 0.,  1.,  2.,  3.],
            [ 4.,  5., 77.,  7.],
            [ 8.,  9., 10., 11.]])
    tensor([[ 0.,  1.,  2.,  3.],
            [ 4.,  5.,  9.,  7.],
            [ 8.,  9., 10., 11.]])


如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。 例如，[0:2, :]访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。 虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。


```python
x[0:2, :] = 12
print(x)
```

    tensor([[12., 12., 12., 12.],
            [12., 12., 12., 12.],
            [ 8.,  9., 10., 11.]])


## 2.1.5. 节省内存
运行一些操作可能会导致为新结果分配内存。 例如，如果我们用Y = X + Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量。

在下面的例子中，我们用Python的id()函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行Y = Y + X后，我们会发现id(Y)指向另一个位置。 这是因为Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置。


```python
before = id(y)
y = y + x
print(id(y) == before)
```

    False


这可能是不可取的，原因有两个：
1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；
2. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。




```python
z = torch.zeros_like(y)
print(z)
before = id(z)
print(before)

z[:] = x + y
print(id(z) == before)
```

    tensor([[0., 0., 0., 0.],
            [0., 0., 0., 0.],
            [0., 0., 0., 0.]])
    1673675734672
    True


如果在后续计算中没有重复使用X， 我们也可以使用X[:] = X + Y或X += Y来减少操作的内存开销。


```python
before = id(x)
x += y
print(id(x) == before)
```

    True


## 2.1.6. 转换为其他Python对象
将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。


```python
a = x.numpy()
b = torch.tensor(a)
print("a:{}".format(type(a)))
print("a:{}".format(type(b)))
```

    a:<class 'numpy.ndarray'>
    a:<class 'torch.Tensor'>


要将大小为1的张量转换为Python标量，我们可以调用item函数或Python的内置函数。



```python
a= torch.tensor([3.5])
print(a)
print(a.item())
print(float(a))
print(int(a))
```

    tensor([3.5000])
    3.5
    3.5
    3


## 2.1.7. 小结
深度学习存储和操作数据的主要接口是张量（n维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象。
