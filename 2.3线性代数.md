## 2.3. 线性代数
### 2.3.1. 标量
![image_122](./assets/Snipaste_2023-12-04_16-22-26.png)


```python
import torch

x = torch.tensor(3.0)
y = torch.tensor(2.0)
print(x + y)
print(x - y)
print(x * y)
print(x / y)
print(x ** y)
```

    tensor(5.)
    tensor(1.)
    tensor(6.)
    tensor(1.5000)
    tensor(9.)


### 2.3.2. 向量
向量可以被视为标量值组成的列表。 这些标量值被称为向量的元素（element）或分量（component）。 当向量表示数据集中的样本时，它们的值具有一定的现实意义。 例如，如果我们正在训练一个模型来预测贷款违约风险，可能会将每个申请人与一个向量相关联， 其分量与其收入、工作年限、过往违约次数和其他因素相对应。 如果我们正在研究医院患者可能面临的心脏病发作风险，可能会用一个向量来表示每个患者， 其分量为最近的生命体征、胆固醇水平、每天运动时间等。 在数学表示法中，向量通常记为粗体、小写的符号 （例如x,y和z).

人们通过一维张量表示向量。一般来说，张量可以具有任意长度，取决于机器的内存限制。


```python
x = torch.arange(4)
print(x)
```

    tensor([0, 1, 2, 3])


我们可以使用下标来引用向量的任一元素，例如可以通过x(i)来引用第i个元素。 注意x(i)元素
是一个标量，所以我们在引用它时不会加粗。 大量文献认为列向量是向量的默认方向，在本书中也是如此。 在数学中，向量
x可以写为：
![iamge-13465465](./assets/Snipaste_2023-12-04_16-37-42.png)
其中x(1)....x(n)是向量的元素。在代码中，我们通过张量的索引来访问任一元素。


```python
print(x[3])
```

    tensor(3)


### 2.3.2.1 长度,维度和形状
![image245adsd](./assets/Snipaste_2023-12-04_16-38-31.png)
当用张量表示一个向量（只有一个轴）时，我们也可以通过.shape属性访问向量的长度。 形状（shape）是一个元素组，列出了张量沿每个轴的长度（维数）。 对于只有一个轴的张量，形状只有一个元素。
请注意，维度（dimension）这个词在不同上下文时往往会有不同的含义，这经常会使人感到困惑。 为了清楚起见，我们在此明确一下： 向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量。 然而，张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。


```python
print(len(x))
print(x.shape)
```


    ---------------------------------------------------------------------------
    
    NameError                                 Traceback (most recent call last)
    
    Cell In[1], line 1
    ----> 1 print(len(x))
          2 print(x.shape)


    NameError: name 'x' is not defined


### 2.3.3. 矩阵

![image-asdasdads](./assets/Snipaste_2023-12-04_16-42-30.png)


```python
A = torch.arange(20).reshape(5, 4)
print(A)
```

![image-asdasdasddasdas](./assets/Snipaste_2023-12-04_17-04-39.png)


```python
print(A.T)
```




```python
B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
print(B)

B_T = B.T
print(B_T)
print(B == B_T)
```


    ---------------------------------------------------------------------------
    
    NameError                                 Traceback (most recent call last)
    
    Cell In[2], line 1
    ----> 1 B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
          2 print(B)
          4 B_T = B.T


    NameError: name 'torch' is not defined


### 2.3.4. 张量
当我们开始处理图像时，张量将变得更加重要，图像以
维数组形式出现， 其中3个轴对应于高度、宽度，以及一个通道（channel）轴， 用于表示颜色通道（红色、绿色和蓝色）。 现在先将高阶张量暂放一边，而是专注学习其基础知识。


```python
X = torch.arange(24).reshape(2, 3, 4)
print(X)
```


    ---------------------------------------------------------------------------
    
    NameError                                 Traceback (most recent call last)
    
    Cell In[3], line 1
    ----> 1 X = torch.arange(24).reshape(2, 3, 4)
          2 print(X)


    NameError: name 'torch' is not defined


### 2.3.5. 张量算法的基本性质
标量、向量、矩阵和任意数量轴的张量（本小节中的“张量”指代数对象）有一些实用的属性。 例如，从按元素操作的定义中可以注意到，任何按元素的一元运算都不会改变其操作数的形状。 同样，给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。 例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。




```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()  # 通过分配新内存，将A的一个副本分配给B
print(A)
print(A + B)
print(A * 2)
```


    ---------------------------------------------------------------------------
    
    NameError                                 Traceback (most recent call last)
    
    Cell In[4], line 1
    ----> 1 A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
          2 B = A.clone()  # 通过分配新内存，将A的一个副本分配给B
          3 print(A)


    NameError: name 'torch' is not defined


![image-Hadamard](./assets/Snipaste_2023-12-04_17-28-34.png)
将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。


```python
a = 2
X = torch.arange(24).reshape(2, 3, 4)
print(a + X)
print((a * X).shape)
print(A * B)
```


    ---------------------------------------------------------------------------
    
    NameError                                 Traceback (most recent call last)
    
    Cell In[5], line 2
          1 a = 2
    ----> 2 X = torch.arange(24).reshape(2, 3, 4)
          3 print(a + X)
          4 print((a * X).shape)


    NameError: name 'torch' is not defined


### 2.3.6. 降维
我们可以对任意张量进行的一个有用的操作是计算其元素的和，数学表示法使用∑符号表示求和,为了表示长度为d的向量中元素的综合，可以记为∑(^d)(_i=1){x_i},在代码中可以调用起算求和的函数。
![ image-Snipaste_2023-12-04_17-28-555.png ](./assets/Snipaste_2023-12-09_15-44-31.png)
默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定axis=0。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。



```python

x = torch.arange(4, dtype=torch.float32)
x, x.sum()
A_sum_axis0 = A.sum(axis=0)
A_sum_axis1 = A.sum(axis=1)
print(A)
print(A_sum_axis0)
print(A_sum_axis0.shape)
print(A_sum_axis1)
print(A_sum_axis1.shape)
```

    tensor([[ 0.,  1.,  2.,  3.],
            [ 4.,  5.,  6.,  7.],
            [ 8.,  9., 10., 11.],
            [12., 13., 14., 15.],
            [16., 17., 18., 19.]])
    tensor([40., 45., 50., 55.])
    torch.Size([4])
    tensor([ 6., 22., 38., 54., 70.])
    torch.Size([5])


沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。


```python
A_sum_axis01 = A.sum(axis=[0, 1])  # 结果和A.sum()相同
print(A_sum_axis01)

```

    tensor(190.)


一个与求和相关的量是平均值（mean或average）。 我们通过将总和除以元素总数来计算平均值。 在代码中，我们可以调用函数来计算任意形状张量的平均值。
mean():求平均
numel():元素个数


```python
print(A.mean())
print(A.sum() / A.numel())
```

    tensor(9.5000)
    tensor(9.5000)


同样，计算平均值的函数也可以沿指定轴降低张量的维度。


```python
print(A)
print(A.sum(axis=0))
print(A.mean(axis=0))
print(A.shape)
print(A.sum(axis=0) / A.shape[0])
```

    tensor([[ 0.,  1.,  2.,  3.],
            [ 4.,  5.,  6.,  7.],
            [ 8.,  9., 10., 11.],
            [12., 13., 14., 15.],
            [16., 17., 18., 19.]])
    tensor([40., 45., 50., 55.])
    tensor([ 8.,  9., 10., 11.])
    torch.Size([5, 4])
    tensor([ 8.,  9., 10., 11.])


#### 2.3.6.1. 非降维求和
但是，有时在调用函数来计算总和或均值时保持轴数不变会很有用。
例如，由于sum_A在对每行进行求和后仍保持两个轴，我们可以通过广播将A除以sum_A。




```python
print(A)
sum_A = A.sum(axis=1, keepdims=True)
print(sum_A)
print(sum_A.shape)

A_sum_A = A / sum_A
print(A_sum_A)

```

    tensor([[ 0.,  1.,  2.,  3.],
            [ 4.,  5.,  6.,  7.],
            [ 8.,  9., 10., 11.],
            [12., 13., 14., 15.],
            [16., 17., 18., 19.]])
    tensor([[ 6.],
            [22.],
            [38.],
            [54.],
            [70.]])
    torch.Size([5, 1])
    tensor([[0.0000, 0.1667, 0.3333, 0.5000],
            [0.1818, 0.2273, 0.2727, 0.3182],
            [0.2105, 0.2368, 0.2632, 0.2895],
            [0.2222, 0.2407, 0.2593, 0.2778],
            [0.2286, 0.2429, 0.2571, 0.2714]])


如果我们想沿某个轴计算A元素的累积总和， 比如axis=0（按行计算），可以调用cumsum函数。 此函数不会沿任何轴降低输入张量的维度。


```python
print(A)
A_cumsum = A.cumsum(axis=0)
print("--------------------")
print(A_cumsum)
```

    tensor([[ 0.,  1.,  2.,  3.],
            [ 4.,  5.,  6.,  7.],
            [ 8.,  9., 10., 11.],
            [12., 13., 14., 15.],
            [16., 17., 18., 19.]])
    --------------------
    tensor([[ 0.,  1.,  2.,  3.],
            [ 4.,  6.,  8., 10.],
            [12., 15., 18., 21.],
            [24., 28., 32., 36.],
            [40., 45., 50., 55.]])


### 2.3.7. 点积（Dot Product）
![image_Snipaste_2023-12-09_10-25-39.png](./assets/Snipaste_2023-12-09_15-44-54.png)

点积计算规则： 对应元素相乘的和
注意，我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积
![image_Snipaste_2023-12-09_11-05-50.png](./assets/Snipaste_2023-12-09_15-45-11.png)


```python
y = torch.ones(4, dtype=torch.float32)
print(x)
print("-----------")
print(y)
print("-----------")
print(torch.dot(x, y))
print("-----------")
print(torch.sum(x * y))
```

    tensor([0., 1., 2., 3.])
    -----------
    tensor([1., 1., 1., 1.])
    -----------
    tensor(6.)
    -----------
    tensor(6.)


### 2.3.8. 矩阵-向量积
![image_Snipaste_2023-12-09_11-08-47.png](./assets/Snipaste_2023-12-09_15-45-27.png)
在代码中使用张量表示矩阵-向量积，我们使用mv函数。 当我们为矩阵A和向量x调用torch.mv(A, x)时，会执行矩阵-向量积。 注意，A的列维数（沿轴1的长度）必须与x的维数（其长度）相同。




```python

A_mv_x = torch.mv(A, x)
print(A_mv_x)
print("---------等同于-------------------")
Ax_sum = (A * x).sum(axis=1)
print(Ax_sum)



```

    tensor([ 14.,  38.,  62.,  86., 110.])
    ---------等同于-------------------
    tensor([ 14.,  38.,  62.,  86., 110.])


### 2.3.9. 矩阵-矩阵乘法
![image_Snipaste_2023-12-09_11-28-40.png](./assets/Snipaste_2023-12-09_15-45-44.png)

矩阵乘法要求： 矩阵A(n*k) 和矩阵B(k*m)他们两个中 矩阵A的列数需要等于矩阵B的行数
矩阵-矩阵乘法可以简单地称为矩阵乘法，不应与“Hadamard积”混淆。



```python
B = torch.ones(4, 3)
A = A
print("A:", A.shape)
print("B:", B.shape)
mm_AB = torch.mm(A, B)

print(mm_AB)



```

    A: torch.Size([5, 4])
    B: torch.Size([4, 3])
    tensor([[ 6.,  6.,  6.],
            [22., 22., 22.],
            [38., 38., 38.],
            [54., 54., 54.],
            [70., 70., 70.]])


### 2.3.10. 范数

线性代数中最有用的一些运算符是范数（norm）。 非正式地说，向量的范数是表示一个向量有多大。 这里考虑的大小（size）概念不涉及维度，而是分量的大小。
![image_Snipaste_2023-12-09_11-37-11.png](./assets/Snipaste_2023-12-09_15-46-03.png)


![image_Snipaste_2023-12-09_11-52-02.png](./assets/Snipaste_2023-12-09_15-46-16.png)
![image_Snipaste_2023-12-09_11-54-20.png](./assets/Snipaste_2023-12-09_15-46-32.png)



```python
u = torch.tensor([3.0, -4.0])
norm = torch.norm(u)
print(norm)
print("------------")
abs_sum = torch.abs(u).sum()
print(abs_sum)
print("------------")
frobenius = torch.norm(torch.ones((4, 9)))
print(frobenius)
```

    tensor(5.)
    ------------
    tensor(7.)
    ------------
    tensor(6.)


### 2.3.10.1. 范数和目标
在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。

### 2.3.11. 关于线性代数的更多信息
仅用一节，我们就教会了阅读本书所需的、用以理解现代深度学习的线性代数。 线性代数还有很多，其中很多数学对于机器学习非常有用。 例如，矩阵可以分解为因子，这些分解可以显示真实世界数据集中的低维结构。 机器学习的整个子领域都侧重于使用矩阵分解及其向高阶张量的泛化，来发现数据集中的结构并解决预测问题。 当开始动手尝试并在真实数据集上应用了有效的机器学习模型，你会更倾向于学习更多数学。 因此，这一节到此结束，本书将在后面介绍更多数学知识。

如果渴望了解有关线性代数的更多信息，可以参考线性代数运算的在线附录(https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html)或其他优秀资源。


```python
# 练习

import torch

a= torch.ones([2,5,4])
print(a)
print(a.shape)

```

    tensor([[[1., 1., 1., 1.],
             [1., 1., 1., 1.],
             [1., 1., 1., 1.],
             [1., 1., 1., 1.],
             [1., 1., 1., 1.]],
    
            [[1., 1., 1., 1.],
             [1., 1., 1., 1.],
             [1., 1., 1., 1.],
             [1., 1., 1., 1.],
             [1., 1., 1., 1.]]])
    torch.Size([2, 5, 4])



```python
print(a.sum(axis=1))
```

    tensor([[5., 5., 5., 5.],
            [5., 5., 5., 5.]])



```python
print(a.sum(axis=0))
```

    tensor([[2., 2., 2., 2.],
            [2., 2., 2., 2.],
            [2., 2., 2., 2.],
            [2., 2., 2., 2.],
            [2., 2., 2., 2.]])



```python
print(a.sum(axis=2))
```

    tensor([[4., 4., 4., 4., 4.],
            [4., 4., 4., 4., 4.]])



```python
print(a.sum(axis=1,keepdims=True))

```

    tensor([[[5., 5., 5., 5.]],
    
            [[5., 5., 5., 5.]]])



```python

```
