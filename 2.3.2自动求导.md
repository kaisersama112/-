![image-Snipaste_2024-01-14_10-22-44.png](./assets/Snipaste_2024-01-14_10-22-44.png)
![image-Snipaste_2024-01-14_10-34-00.png](./assets/Snipaste_2024-01-14_10-34-00.png)
![image-Snipaste_2024-01-14_10-34-29.png](./assets/Snipaste_2024-01-14_10-34-29.png)
![image-Snipaste_2024-01-14_10-35-30.png](./aseets/Snipaste_2024-01-14_10-35-30.png)
![image-Snipaste_2024-01-14_10-38-14.png](./assets/Snipaste_2024-01-14_10-38-14.png)
![image-Snipaste_2024-01-14_10-38-51.png](./assets/Snipaste_2024-01-14_10-38-51.png)
![image-Snipaste_2024-01-14_10-40-09.png](./assets/Snipaste_2024-01-14_10-40-09.png)

## 自动求导
假设我们想对函数y=(2x^T)x 关于列向量x求导



```python
import torch
x= torch.arange(4.0)
x
```




    tensor([0., 1., 2., 3.])



**在我们计算y关于x的梯度之前，我们需要一个地方来存储梯度**


```python
x.requires_grad_(True) # 等价于x=torch.arange(4.0,requires_grad=True)
x.grad
```

**现在让我们计算y**


```python
y= 2*torch.dot(x,x)
y
```




    tensor(28., grad_fn=<MulBackward0>)



**通过调用反向传播函数来自动计算y关于x每个分量的梯度**



```python
y.backward()
x.grad
```




    tensor([ 0.,  4.,  8., 12.])




```python
x.grad==4*x
```




    tensor([True, True, True, True])



**现在让我们计算x的另一个函数**
 


```python
# 在默认情况下，pytorch会累积梯度，我们需要清楚之前的值
x.grad.zero_()
y=x.sum()
y.backward()
#向量x求和相当于向量x乘一个单位向量E，那么y对x求导后，就是y'=E
x.grad
```




    tensor([1., 1., 1., 1.])



深度学习中，我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和



```python
# 对非标量调用`backward`需要传入一个`gradient`参数，该参数指定微分函数是为了把张量对
#张量的求导转换为标量对张量的求导。

x.grad.zero_()
y=x*x
# 等价于y.gradward(torch.ones(len(x)))
y.sum().backward()
x.grad
```




    tensor([ 0.,  4.,  8., 12.])



将某些计算移动到记录的计算图之外


```python
x.grad.zero_()
y=x*x

u=y.detach()
z= u*x
z.sum().backward()
# 这里可以理解为对x求偏导 所以需要将u看作为一个常数，.detach() 可以阻止梯度回传
x.grad==u

```




    tensor([True, True, True, True])




```python
x.grad.zero_()
y.sum().backward()
x.grad==2*x
x.grad
```




    tensor([0., 2., 4., 6.])



即使构建函数的计算图需要通过python控制流（例如，条件，循环或任意函数调用），我们任然可以计算得到的变量的梯度



```python
def f(a):
    b=a*2
    while b.norm()<1000:
        b=b*2
    if b.sum()>0:
        c=b
    else:
        c= 100*b
    return c
a= torch.randn(size=(),requires_grad=True)
print(a)
d=f(a)
d.backward()
a.grad
```

    tensor(-0.7648, requires_grad=True)
    




    tensor(204800.)




```python

```


```python

```


```python

```


```python

```
