# Ai入门指南

# 一、什么是Ai？

AI是人工智能（Artificial Intelligence）的缩写，从技术的角度来看，**人工智能**涵盖了**机器学习**，而**深度学习**属于**机器学习**更深入的研究。

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/7ad53aa7-09d7-4b9f-a4c2-8d67a14fc5f2.png)

**机器学习**

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/f6f241d2-4251-4835-b96f-4085d730daee.png)

机器学习是**让计算机通过数据学习，而不是直接编程去执行某个任务的过程。**就是计算机**从经验中“学习”**，以便在没有明确指令的情况下做出决策或预测。

举个例子，想象你在教一只狗坐下。你不会每次都给它详细的指令，而是通过重复训练、奖励（比如给零食）来让它明白这个动作。机器学习就像这种训练过程，计算机通过分析大量的数据（比如图片、文字等）来找出规律，从而在遇到新的数据时作出相应的反应。

机器学习可以分为几个主要类型：

1. **监督学习**：给计算机输入数据和对应的标签（答案），它学习如何从输入预测输出，比如**分类**和**回归**问题。
    
2. **无监督学习**：只有输入数据，没有标签。计算机自己寻找数据中的模式，比如**聚类**和**降维**。
    
3. **强化学习**：通过与环境互动来学习，计算机根据反馈（奖励或惩罚）来调整自己的行为，比如训练游戏中的角色。
    

**深度学习**

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/7dbb06d8-932b-4a3b-b1c9-63341f660703.png)

深度学习是机器学习的一个分支，**主要关注使用神经网络来处理数据。它模仿人脑的工作方式，通过层层神经元来学习和提取特征，能够处理更复杂的数据和任务。**

深度学习就像是一座多层的“黑箱”模型。每一层都能提取出数据中的不同特征，越往后，提取的特征越复杂。在图像识别中，第一层可能识别边缘，第二层可能识别形状，而更深的层可能识别出人脸或物体。

关键点包括：

1. **多层神经网络**：使用多个层级的神经元来学习，层数越多，模型越复杂，能处理更高维度的数据。
    
2. **大量数据**：深度学习通常需要大量的数据进行训练，以便神经网络能够学习到足够多的特征。
    
3. **强大的计算能力**：深度学习需要强大的计算资源来加速训练过程。
    
4. **应用**：图像识别、自然语言处理、语音识别。
    

# 二、机器学习介绍

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/c3b8dc6d-4e88-47bb-935d-bd05c2edeba1.jpg)

**监督学习**

监督学习（Supervised Learning ）使用标记数据集来训练算法，以便训练后的算法可以对数据进行分类或准确预测结果。在监督学习中，**每个样本数据都被正确地标记过**。算法模型在训练过程中通过一系列的误差矫正，不断学习拟合到标记值，因此得名为“监督”学习。主要任务：分类问题，回归问题。

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/236d3e62-b9e9-47d9-8e16-1fa3f5967a59.png)

**无监督学习**

无监督学习（Unsupervised Learning）用算法来分析并聚类未标记的数据集，以便发现数据中隐藏的模式和规律，而不需要人工干预（因此，被称为“无监督的”学习）。无监督学习模型用于三个主要任务: 聚类问题、关联问题和降维问题：

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/450da91c-491f-437d-b4e9-bc9858408ad4.png)

# 三、最简单的机器学习模型

**单变量线性回归**

代码：
[请至钉钉文档查看附件《线性回归.html》](https://alidocs.dingtalk.com/i/nodes/mExel2BLV54XNjrgfXyrDk3LWgk9rpMq?doc_type=wiki_doc&iframeQuery=anchorId%3DX02m38i37p9nqpjrpors1&rnd=0.9594798160871312)

**线性回归：**

指因变量(y)与自变量(x)之间存在线性关系，我们可以用某一线性回归模型来拟合因变量与自变量的数值，并采用某种估计方法来确定模型的有关参数来得到具体的回归方程。(y=ax+b)

### 单变量线性回归

是两个变量之间建立的线性方程的拟合模型，用一个变量去预测另一个变量。

**理解：**

数据：

|面积（m^2）|价格(万元)|
|---|---|
|50|85|
|70|115|
|80|128|
|52|66|
|62|111|
|88|111|
|90|142|
|...|....|

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/558b3500-c737-4e4d-9585-d56948f67631.png)

现在设想一下 ，如果我们想通过面积去预测最终的价格是否可以建立一个函数用来预测：y=kx+b，为了使函数具有更好的拟合效果 我们加了一个偏置（常数b）

我们改写一下上面函数的形式：h\_{\emptyset }(x)={\emptyset}\_{0}+{\emptyset}\_{1}x,在这里他的函数性质与y=kx+b是一致的。

k**，**{\emptyset}\_{1}**：斜率**

b**，**{\emptyset}\_{0}**：截距**

y**，**h\_{\emptyset }(x)**：预测值**

为了能够成功预测最后的价格我们需要找到一条线段 这条线段能最大程度的拟合这些数据点

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/5e6b1cc3-d75c-4f1d-ab81-e85f0b886978.png)

而我们现在的问题就是这条线该怎么去找？什么样的线才合适？

为了解决这个问题，我们引入了代价函数

### 损失函数（代价函数）

通过对上述的分析，我们可以知道所找的这条线应该接近大部分的样本点，即找到的该直线应该与样本点之间的距离最短，如下：

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/8f0df297-9d4f-4a82-a1a9-8aa503f6b687.png)

为了找一条直线，使得所有样本点与直线之间的距离最小，我们已经知道该直线的模型，即：h\_{\emptyset }(x)={\emptyset}\_{0}+{\emptyset}\_{1}x，那单个样本的误差大小我们就可以表示为d=h\_{\emptyset }(x)-y，那么单个样本的误差公式我们就可以写为：J^{i}(x,y)=h\_{\emptyset }(x)-y那对于所有样本呢？我们采用D= {\textstyle \sum\_{i=1}^{m}}{h\_{\emptyset }({x\_i})-y\_{i}}来表示。这样对吗？思考一下,我们观察图像，对于误差距离是存在正负值的，如果我们只是简单的累加求和是否会存在误差抵消？那有什么办法能够解决这个问题呢？为了解决这个问题,我们对误差取平方再进行除法操作最后的误差公式就变为：J^{i}(x,y)=\frac{1}{2}{(h\_{\emptyset }(x)-y)^2}，

为了更好的评估误差值，一般大家采用平均绝对误差（Mean Absolute Error, MAE）和均方误差（Mean Squared Error, MSE）这两种广泛使用，它们都用于衡量预测值与实际值之间的差异。

#### 均方误差（Mean Squared Error, MSE）

所有预测误差的平方和的平均值\text{MSE} = \frac{1}{n} \sum\_{i=1}^{n} (y\_i - \hat{y}\_i)^2

带入我们的例子：\text{MSE} = {\frac{1}{n}} {{\textstyle \sum\_{i=1}^{m}}{({h\_{\emptyset }({x\_i})}-{y\_{i}}}})^2

#### 平均绝对值误差（Mean Absolute Error MAE）

所有预测误差的绝对值之和的平均值\text{MAE} = \frac{1}{n} \sum\_{i=1}^{n} |y\_i - \hat{y}\_i|

带入我们的例子：\text{MAE} = {\frac{1}{n}} {{\textstyle \sum\_{i=1}^{m}}{|{h\_{\emptyset }({x\_i})}-{y\_{i}}}}|

### 梯度下降

现在我们能够根据损失函数来衡量我们函数的预测值与真实值之间的差距了，那我们需要做的就是如何让预测值与真实值的误差最小。

我们回顾一下我们之前提到的原始函数，以及损失函数：

**原始函数：**h\_{\emptyset }(x)={\emptyset}\_{0}+{\emptyset}\_{1}x 

**损失函数：**J^{i}(x,y)=\frac{1}{2}{(h\_{\emptyset }(x)-y)^2}

在原始函数中我们可以看到两个未知变量（{\emptyset}\_{0},{\emptyset}\_{1}），通过改变这两个变量，都可以达到对误差值最小化。为了能够找到最优解（这里不讨论局部最优解，线性回归函数不存在局部最优），那这里的损失函数也应该是针对这两个变量做误差最小化，那损失函数就可以改写为下面这种形式：

**针对单个数据：**J^{i}({\emptyset}\_{0},{\emptyset}\_{1})=\frac{1}{2}{(h\_{\emptyset }(x)-y)^2}

**推广到整个数据：**J({\emptyset}\_{0},{\emptyset}\_{1}) = {\frac{1}{n}} {{\textstyle \sum\_{i=1}^{m}}{({h\_{\emptyset }({x\_i})}-{y\_{i}}}})^2 

在这里梯度下降的出现是为了将J({\emptyset}\_{0},{\emptyset}\_{1}) = {\frac{1}{n}} {{\textstyle \sum\_{i=1}^{m}}{({h\_{\emptyset }({x\_i})}-{y\_{i}}}})^2 的损失值降到最小。

为了能够理解梯度下降，需要先了解几个前置知识点：**二元函数，偏导数**

#### 二元函数的几何意义

二元函数表示空间直角坐标系中的一个曲面，每一个自变量（x，y）都唯一对应一个因变量z。

比如 z=x^2+y^2,曲面图像如下：

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/8711f446-9181-4e6f-b94f-02fd3f06daed.png)

比如 z=exp(-x^2-y^2)图像如下：

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/cf0db741-b7d4-49b5-8cbb-80c2ee9a4ebd.png)

#### 偏导数的几何意义

对于一元函数的导数我们是知道的，例如：函数y=2x的导数为2,这里的2是函数的斜率。对于二元函数我们同样要研究它的“变化率”。

**几何意义：**

假设有一个多变量函数 f(x\_1, x\_2, \dots, x\_n)，它定义在一个 n维空间中。对于函数 f(x\_1,x\_2)，我们可以把它看作是一个三维的曲面：函数的值f(x1,x2)是平面上的高度。

- **偏导数** \frac{\partial {f}}{\partial{x\_{i}} }代表了函数在某一点\text{比如}(x\_1, x\_2, \dots,x\_{i-1},x\_{i+1}, x\_n)处，沿着x\_i方向的变化率。几何上，这可以理解为该点所在的切平面在 x\_i方向上的斜率。
    

假设我们固定其他变量（比如 x\_1,x\_2,…,x\_i−1,x\_i+1,x\_n），只在x\_i方向上变化。偏导数\frac{\partial {f}}{\partial{x\_{i}} }就表示当我们沿着x\_i方向移动时，函数值的变化速率。

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/eaee1273-8471-4d3e-bb41-f0ffb2a8b878.png)![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/f8d8eb01-6693-4fc9-9b37-6591cd035d12.png)

- **正偏导数**：当某个**偏导数为正时，说明沿着该方向移动，函数值增加。**几何上沿着该方向走会爬**上坡**。
    
- **负偏导数**：当某个**偏导数为负时，说明沿着该方向移动，函数值减小。**几何上沿着该方向走会走**下坡**。
    
- **偏导数为零**：当某个偏导数为零时，说明在该方向上，函数值没有变化。几何上，这时沿着该方向走，曲面不会上升或下降，可能是一个平坦的区域。
    

#### 梯度下降法

|面积（m^2）|价格(万元)|
|---|---|
|50|85|
|70|115|
|80|128|
|52|66|
|62|111|
|88|111|
|90|142|
|...|....|

回到最开始我们的问题上面来，原始函数：h\_{\emptyset }(x)={\emptyset}\_{0}+{\emptyset}\_{1}x <=>y=kx+b,我们为了能够衡量我们的预测值与真实值之间的差距我们引入了损失函数J({\emptyset}\_{0},{\emptyset}\_{1}) = {\frac{1}{n}} {{\textstyle \sum\_{i=1}^{m}}{({h\_{\emptyset }({x\_i})}-{y\_{i}}}})^2，而我们为了能够能够降低损失函数预测值与真实值之间的差距，我们又引入了梯度下降这个概念。

梯度下降的数学公式：\theta\_{n+1}= \theta\_{n}- \eta \cdot \nabla J \left( \theta \right) （{(x\_{n+1},y\_{n+1})}={(x\_n,y\_n)}-{\eta} \cdot { \nabla J ( x,y )}）

\theta\_{n+1}： 下一个值（神经网络中参数更新后的值）

\theta\_{n}：当前值（当前参数值）

\eta：学习率或步长，控制每一步走的距离（需要手动调整的超参数）

\nabla：梯度，函数当前位置的最快上升点（梯度向量指向上坡，负梯度向量指向下坡）

J( \theta )：函数（等待优化的目标函数）

这里我们先假设一个简单一点的损失函数J(x,y)={x}^2+2{y}^2 这个函数形式同样具有两个因变量（**在这里**x,y**都表示为误差值不要与原始函数混淆**），我们通过这个函数我们来描述梯度下降的优化过程。

首先我们先求解函数的梯度（对每一项参数求偏导）

\frac{ \partial J (x ,y )}{ \partial x}=2x 

\frac{ \partial J ( x,y )}{ \partial y}=4y 

然后我们带入参数值：这边先假设参数值（\theta\_{n}）为：a(3,3) ,学习率（\eta）为：0.1

\begin{align} {(x\_{n+1},y\_{n+1})}={(x\_n,y\_n)}-{\eta} \cdot { \nabla J ( x,y )} \\ ={(x\_n,y\_n)}-{\eta} \cdot { (2x,4y)} \end{align}

##### 计算过程

**第一次迭代：**(x\_{n+1},y\_{n+1})  =(-3,-3)-0.1(2\*-3,4\*-3)  =(-3+0.6,-3+1.2) =(-2.4,-1.8)

将值带入损失函数J(x,y)={x}^2+2{y}^2中 J(x,y)={(-2.4)}^2+2\*({-1.8})^2=12.24

**第二次迭代：**

(x\_{n+1},y\_{n+1})  =(-2.4,-1.8)-0.1(2\*-2.4,4\*-1.8)  =(-1.92,-1.08)

将值带入损失函数中J(x,y)={(-1.92)}^2+2\*({-1.08})^2=6.0192

经过不断的迭代，我们可以发现误差在不断的减小。

##### 梯度下降核心思想

- 计算损失函数对参数的梯度（即偏导数）。
    
- 按照梯度的反方向调整参数，以减少损失函数的值。
    
- 通过多次迭代更新，模型的参数会逐渐趋向最优解。
    

![v2-451f15e18a969fc1a4c64707205832ec_r.jpeg](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/612335e7-92c5-44fa-b1ab-746a608fc685.jpeg)

#### 批量梯度下降法（BGD，Batch Gradient Descent）

回顾之前所讲的梯度下降法公式：\theta\_{n+1}= \theta\_{n}- \eta \cdot \nabla J \left( \theta \right) ，而所谓批量梯度下降（BGD，Batch Gradient Descent）它的具体思路是在更新每一参数时都使用所有的样本来进行梯度的更新。我们将我们上面的公式推广到多个样本中:

\theta\_{n+1}= \theta\_{n}- \eta\cdot \frac{1}{m}  \sum\_{i=1}^{n} \nabla J({\theta\_n};X^{i},Y^{i} )

m 是训练集的样本数量。

X^{(i)} 和 Y^{(i)}分别是第 i个样本的输入和标签。

∇J(θ\_n;X^{(i)},Y^{(i)}) 是损失函数 J(\theta) 对于每个样本的梯度。

这里我们假设损失函数 J(\theta)是基于均方误差（MSE）的损失函数，那么对参数\theta的梯度通常是：

\nabla {J}(\theta )= {\frac{1}{m}}{\sum\_{i=1}^{m}(H\_\theta(X^{(i)})-Y^{(i)})}X^{(i)}

H\_θ(X^{(i)}) ：模型对于第i个样本的预测

Y^{(i)}：第i个训练样本的真实标签

X^{(i)}：第i个训练样本的特征向量

结合起来，批量梯度下降的更新规则就是：

\theta\_{n+1}= \theta\_{n}-{\eta} {\cdot}\frac{1}{m} \sum\_{i=1}^{m}  ({H\_{\theta}}( X^{(i)})-Y^{(i)}) {x}^{(i)}

这就是基于整个数据集计算的梯度下降更新规则，目标是通过所有样本的信息来更新参数\theta。**批量梯度下降通过使用整个数据集的梯度信息来更准确地更新参数，因此相较于SGD，收敛较为平稳，但每次更新的计算开销较大，通常需要更多的计算资源和时间。**

#### 随机梯度下降法（SGD，Stochastic Gradient Descent）

随机梯度下降法（SGD，Stochastic Gradient Descent）是一种基于单个样本进行参数更新的优化方法，与批量梯度下降（Batch Gradient Descent）相比，SGD在每次迭代中仅计算一个样本的梯度，从而使得每次参数更新的计算量较小，且可以更快地迭代。但是，由于其更新基于单个样本，收敛过程通常会比批量梯度下降更加波动。

在梯度下降的背景下，使用样本(X^{(i)}, Y^{(i)}) 的梯度来更新参数的方式就是 **随机梯度下降**，它的更新公式是：

\theta\_{n+1}= \theta\_{n}- \eta\cdot  \nabla J({\theta\_n};X^{i},Y^{i} )

其中\theta\_n 是第n次迭代的参数，\eta是学习率，\nabla J(\theta\_n; X^{(i)}, Y^{(i)})是基于第i个样本的损失函数的梯度。

在这里我们同样假设损失函数 J(\theta)是基于均方误差（MSE）的损失函数

J(\theta) = \frac{1}{2m} \sum\_{i=1}^{m} (H\_{\theta}(X^{(i)}) - Y^{(i)})^2

其中，H\_{\theta}(X^{(i)})是模型的预测，Y^{(i)} 是真实值，m 是样本数。对于一个样本(X^{(i)}, Y^{(i)})，损失函数关于参数 \theta\_j的梯度为：\nabla J(\theta\_n; X^{(i)}, Y^{(i)}) = (H\_{\theta}(X^{(i)}) - Y^{(i)}) \cdot X^{(i)}\_j

因此，SGD 的更新公式就变成：\theta\_j = \theta\_j - \eta \cdot (H\_{\theta}(X^{(i)}) - Y^{(i)}) \cdot X^{(i)}\_j

##### 迭代过程

1. **随机选择一个样本**：在每次迭代中，从数据集中随机选择一个样本 (X^{(i)}, Y^{(i)})。
    
2. **计算梯度**：基于当前参数\theta\_n 和所选样本((X^{(i)}, Y^{(i)})，计算损失函数的梯度\nabla J(\theta\_n; X^{(i)}, Y^{(i)})
    
3. **更新参数**：使用计算得到的梯度和学习率 \eta 更新参数 \theta\_j。
    

这种方式在每次迭代时，只考虑单个样本的数据，避免了批量梯度下降所需要的庞大计算量。尽管SGD的更新可能较为震荡，但它有较好的表现，尤其在数据量很大的时候。

##### 随机梯度下降的特点

- **单个样本更新**：与批量梯度下降不同，SGD 在每次更新中只用一个样本 (X^{(i)}, Y^{(i)})来计算梯度并更新参数，而不是使用所有样本。
    
- **噪声和波动**：由于每次更新都基于一个样本的梯度，SGD 在每次迭代时的更新方向可能会有所不同，从而导致参数更新的路径呈现出更多的波动。虽然这种波动可能让收敛过程变得不稳定，但它也使得SGD能够逃脱局部最优解，并能在大规模数据集上快速收敛。
    
- **计算效率**：由于每次迭代只考虑一个样本，因此每次更新所需的计算量非常小，SGD适用于数据量非常大的问题。
    

#### 小批量随机梯度下降（MBGD）（略）

### 总结

在单变量线性回归当中，我们为了能够通过面积去预测价格，我们通过因变量(y)与自变量(x)建立了一个在线性关系的函数：h\_{\emptyset }(x)={\emptyset}\_{0}+{\emptyset}\_{1}x,为了能够更好的衡量我们函数预测值与真实数据的一个误差，我们又引入了损失函数：J({\emptyset}\_{0},{\emptyset}\_{1}) = {\frac{1}{n}} {{\textstyle \sum\_{i=1}^{m}}{({h\_{\emptyset }({x\_i})}-{y\_{i}}}})^2,最后我们为了能够将预测值与真实值之间的误差减小，我们使用了梯度下降算法以及学习率这个概念：\theta\_{n+1}= \theta\_{n}- \eta \cdot \nabla J \left( \theta \right)。我们通过对权重，偏置求偏导，从而不断的减少误差值达到一个最佳的值。

**多变量线性回归(略)**

**逻辑回归（略）**

**多层感知机模型（内容：略）**

代码：

[请至钉钉文档查看附件《多层感知机.html》](https://alidocs.dingtalk.com/i/nodes/mExel2BLV54XNjrgfXyrDk3LWgk9rpMq?doc_type=wiki_doc&iframeQuery=anchorId%3DX02m38iybl0hzdaroxzdmo&rnd=0.9594798160871312)

# 四、自然语言入门(引出大模型前置概念)

## 1. 神经网络基础

代码：

[请至钉钉文档查看附件《卷积神经.html》](https://alidocs.dingtalk.com/i/nodes/mExel2BLV54XNjrgfXyrDk3LWgk9rpMq?doc_type=wiki_doc&iframeQuery=anchorId%3DX02m38iwkfxcmwqghnk7yo&rnd=0.9594798160871312)

- **神经元与激活函数**
    
- **前馈神经网络 (Feedforward Neural Networks)**
    

### 1.1 为什么需要神经网络？

神经网络的出现，实际上是为了解决更为复杂的预测和分类问题。其发展和线性回归有很大的关系，下面我们从单变量线性回归出发，逐步引入神经网络的概念。

首先，我们回顾一下最开始我们的线性回归 ，他是一种用于预测目标变量与自变量之间关系的统计方法。对于单变量线性回归，它的模型为：y = wx + b

- y 是目标变量。
    
- x 是输入特征。
    
- w 是权重，表示输入特征对预测的影响。
    
- b 是偏置项，用来调整输出的值。
    

在这种模型下，我们假设目标变量 y 与输入变量 x 存在一种线性关系。训练过程的核心任务是找到最优的权重 w 和偏置 b，使得预测值与真实值之间的误差最小。

随着问题复杂性的增加，输入特征不再局限于一个变量。比如，我们可能需要预测一个目标变量 y 基于多个特征 x\_1, x\_2, \dots, x\_n​。此时我们引入了多变量线性回归，我们的模型变为了：y = w\_1 x\_1 + w\_2 x\_2 + \dots + w\_n x\_n + b,这里，目标变量 y 是多个输入特征的线性组合，权重和偏置项也随着特征数量增加而扩展。

**单变量或多变量的线性回归模型假设了输入与输出之间的线性关系，但现实世界中的许多问题并不符合这一假设，关系往往是非线性的。**为了处理这类问题，我们需要更复杂的模型。

神经网络的引入正是为了解决这一问题。**神经网络通过引入激活函数，使得模型能够拟合输入与输出之间的非线性关系。**

### 1.2 神经元与激活函数

#### 1.2.1  神经元的工作原理

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/204e7bab-e02c-4ee0-9720-ef748f0fba1e.png)

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/3fe60b60-ff69-49d0-9b34-e2ce33a9ed7d.png)

神经元是神经网络的基本单元，它模仿生物神经元的工作方式。每个神经元通过接收来自其他神经元的输入信号、进行加权和偏置计算，再通过激活函数输出信号。步骤如下：

1. **输入**：神经元接收来自上一层的输入信号，记作x\_1, x\_2, \dots, x\_n，这些信号通常来自其他神经元的输出。
    
2. **加权和**：每个输入信号都有一个对应的权重 w\_1, w\_2, \dots, w\_n​，神经元会将输入信号与权重相乘，计算加权和。并且加上一个偏置项 b，公式为： z = w\_1 x\_1 + w\_2 x\_2 + \dots + w\_n x\_n + b
    
3. **激活函数**：加权和 z 会输入到激活函数（Activation Function）中，激活函数决定了神经元的输出信号 a。它通常是一个非线性函数，输出为：a = f(z)
    
4. **输出**：神经元将这个输出 a 传递给下一层的神经元，或者作为最终结果输出。
    

神经元主要作用就是学习输入信号与输出之间的映射关系，尤其是通过调整权重和偏置来优化模型。

#### 1.2.2  激活函数的作用

激活函数的作用是将神经元的加权和（线性组合）通过一个非线性函数映射到输出空间。这一非线性的引入是神经网络能够学习复杂模式和逼近复杂函数的关键。

##### 激活函数为什么能将线性转换为非线性？

没有激活函数的情况下，神经元的输出是输入的线性加权和。假设输入为 x\_1, x\_2, \dots, x\_n，对应的权重为w\_1, w\_2, \dots, w\_n，偏置为 b，那么加权和就是：z = w\_1 x\_1 + w\_2 x\_2 + \dots + w\_n x\_n + b

这个公式表示的就是输入和权重的线性组合。如果我们将多个神经元按层堆叠，依然是这种加权和的线性组合。没有非线性因素，网络的能力是有限的，因为无论多深的网络，最终它们的输出都可以通过简单的加权求和来表示。

当我们在每个神经元中加入激活函数时，激活函数会对加权和 zzz 进行非线性变换。例如，常见的激活函数如 **Sigmoid**、**Tanh** 和 **ReLU** 都是非线性函数。假设我们用激活函数 f(z) 来处理加权和 z，输出就变成了：a = f(z) = f(w\_1 x\_1 + w\_2 x\_2 + \dots + w\_n x\_n + b)

这时候，输出 a 就不再是线性函数了。通过这种非线性映射，神经网络能够学习输入与输出之间的复杂关系。

##### 例子：没有激活函数时的限制

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/5b03c8de-06fd-4b00-8cfa-272ba72b6e20.jpg)

假设我们没有使用激活函数，仅仅使用线性变换。那么，一个多层的神经网络的行为就等同于一个单一的线性变换。具体来说，假设我们有两层线性神经网络：

- 第一层：输出z\_1 = W\_1 X + b\_1
    
- 第二层：输出z\_2 = W\_2 z\_1 + b\_2
    

代入得到：z\_2 = W\_2 (W\_1 X + b\_1) + b\_2 = (W\_2 W\_1) X + (W\_2 b\_1 + b\_2)

可以看到，第二层的输出z\_2 依然是输入 X 的一个线性组合。无论网络有多少层，最终的输出都只是输入X 的一个线性变换。因此，多个线性层并不会使得网络能够表示更复杂的函数。

**引入非线性的激活函数后：**

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/63cd0669-bc3e-4ce9-912e-06c6b0ddcbcd.jpg)

如果在每一层引入了非线性激活函数，如 ReLU、Sigmoid 或 Tanh，网络就能够表示更复杂的关系。比如在第一层后加入激活函数 f 后，输出就变成了：a\_1 = f(W\_1 X + b\_1)

第二层的输出也通过激活函数进行处理：a\_2 = f(W\_2 a\_1 + b\_2)

此时，神经网络可以通过堆叠多个非线性变换，组合成一个非常复杂的映射。神经网络能够通过这种方式学习到输入和输出之间的非线性关系，具有更强的拟合能力。

#### 1.2.3常见的激活函数

![111111.jpeg](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/0f99cbc1-8d3a-4304-967d-d667b865e77e.jpeg)

##### Sigmoid 函数

用于二分类问题。

原始函数图像：

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/1c6535fb-a5b6-4409-8724-8efa15024062.png)

导数图像：

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/3e56997d-05d5-4404-a7e7-7bd217fb512d.png)

Sigmoid 函数常用于二分类任务中，将输入压缩到(0, 1) 区间，适用于概率输出。

\text{Sigmoid}(z) = \frac{1}{1 + e^{-z}}

- 优点：输出平滑，适合二分类任务。
    
- 缺点：容易出现梯度消失问题。
    

##### Tanh 函数

输出值在-1到1之间。

原始函数图像：

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/94dc5acd-2c65-40f0-bf91-9eade72e3248.png)

导数图像：

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/657e3104-b49a-437f-8fae-c1a1d0ed1930.png)

Tanh 函数与 Sigmoid 类似，但它的输出范围是 (-1, 1)，因此它对输入的变化更加敏感。

\text{Tanh}(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}

- 优点：比 Sigmoid 输出范围更广，且零均值输出有利于训练。
    
- 缺点：同样会遇到梯度消失问题。
    

##### ReLU 函数

常用于深度学习模型中，具有更好的训练效果。

原始函数图像：

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/a1177f48-8d53-42a9-b15e-76ecb40c8359.png)

导数图像：

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/1ede6254-ac65-4665-90cd-bc45d5291e7b.png)

ReLU 函数在当前深度学习中非常常用，它的输出是 \max(0, z)，即输入小于 0 时输出为 0，大于 0 时输出为输入值。

\text{ReLU}(z) = \max(0, z)

- 优点：计算效率高，且避免了梯度消失问题。
    
- 缺点：当输入为负值时，ReLU 会“死亡”（即输出为 0，不再更新）。
    

### 1.3 前馈神经网络 (Feedforward Neural Networks)

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/9cb00d58-39d1-4f88-baac-5787190fc247.png)

前馈神经网络（Feedforward Neural Network, FFNN）是最基础的神经网络类型，它的工作原理非常简单。信息在网络中是单向传播的，即从输入层开始，通过中间的隐藏层（如果有的话），最终到达输出层。其他类型的神经网络目前我们先不做了解。后面会讲到。

#### 1.3.1 前馈神经网络的结构

一个前馈神经网络通常包含以下几个层次：

1. **输入层**：接收外部输入数据，每个神经元对应输入数据的一个特征。
    
2. **隐藏层**：一个或多个层，负责从输入数据中提取特征并进行处理。每个隐藏层中的神经元通过加权和、偏置和激活函数计算输出。
    
3. **输出层**：根据处理结果输出最终预测值或分类结果。在回归任务中，输出层通常只有一个神经元；在分类任务中，输出层的神经元数目等于类别数。
    

#### 1.3.2 计算过程

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/478557c8-5b2c-4ab7-b8ed-49809b6649c9.png)

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/4cf8cc2c-68e9-4b26-9b87-e3d6e372b616.png)

前馈神经网络的计算过程如下：

1. **输入层到隐藏层**：输入数据经过权重和偏置加权求和，然后通过激活函数处理，得到隐藏层的输出。
    
2. **隐藏层到输出层**：隐藏层的输出作为输入传递给输出层，输出层会进行相同的计算，得到最终的输出。
    

举个例子，假设有一个两层的前馈神经网络（输入层、隐藏层和输出层）：

- **输入层**：假设输入为 x = \[x\_1, x\_2, x\_3\]。
    
- **隐藏层**：假设隐藏层有两个神经元，输出为 h = \[h\_1, h\_2\]，每个神经元的输出为：h\_1 = f(w\_{11}x\_1 + w\_{12}x\_2 + w\_{13}x\_3 + b\_1)，h\_2 = f(w\_{21}x\_1 + w\_{22}x\_2 + w\_{23}x\_3 + b\_2)
    

其中 w\_{ij}是权重，b 是偏置，f 是激活函数。

- **输出层**：假设输出层有一个神经元，输出为 y，计算方式为：y = f(w\_1 h\_1 + w\_2 h\_2 + b)
    

其中 w\_1​ ,w\_2是权重，b 是偏置。

## 2. 从梯度下降推广到反向传播

- **反向传播 (Backpropagation)**
    
- **优化算法（单变量线性回归中介绍的梯度下降算法）**
    

在前面的单变量线性回归中，我们讲解了梯度下降的作用以及他的一个实际推导过程：

- 初始化网络的权重和偏置。
    
- 计算当前模型的输出和损失。
    
- 计算损失函数相对于每个参数的梯度（即损失函数的导数）。
    
- 使用梯度下降规则更新参数： \theta = \theta - \eta \cdot \nabla\_\theta L(\theta)其中，\theta 是模型的参数（权重或偏置），\eta 是学习率，\nabla\_\theta L(\theta)是损失函数对参数的梯度。
    

那什么又是反向传播呢？反向传播是用于训练神经网络的核心算法。它是基于梯度下降的思想，使得我们能够高效地计算损失函数相对于每个参数的梯度，从而更新参数。

反向传播的核心思想是**通过链式法则（Chain Rule）计算损失函数相对于每个神经元的梯度**，然后通过这些梯度来更新神经网络的参数。

### 2.1 复合函数求导（链式法则）

微积分中的链式法则（为了不与概率中的链式法则相混淆）用于计复合函数的导数。反向传播是一种计算链式法则的算法，使用高效的特定运输顺序。

设 x是实数， f 和 g 是从实数映射到实数的函数。假设y=g(x) 并且 z=f(g(x))=f(y) 。那么链式法则就是： \frac{dz}{dx} =\frac{dz}{dy} \frac{dy}{dx}。

示例：求f(x)=sin(x^{3}+5)的导数

根据链式法则求导的基本步骤先对函数进行分解：

得： f(x)=sin(x),g(x)=x^{3}+5

接着求里面函数代入外面函数值的导：

f^{ \prime} (g(x))= \cos(g(x))= \cos(x^{3}+5)

求里面函数的导：

g^{ \prime} ( x)=3x^{2}

根据链式法则求导公式得：

\frac{dy}{dx}=f^{\prime} (g(x)) g^{\prime} (x)= \cos(x^{3}+5) \cdot 3x^{2}

### 2.2 反向传播 (Backpropagation ，BP算法)

神经网络的反向传播可以分为2个步骤（计算误差，更新权重），下面将对这2个步骤分别进行说明。

#### 2.2.1 计算误差（损失函数食用）

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/9295e2ef-81ce-4726-8350-d2688db4c1cd.gif)

图中y为我们神经网络的预测值，由于这个预测值不一定正确，所以我们需要将神经网络的预测值和对应数据的标签来比较，计算出误差。误差的计算有很多方法(线性回归中用到的损失函数在这里同样适用)。计算出的误差记为δ.

反向传播，是从后向前传播的一种方法。因此计算完误差后，需要将这个误差向不断的向前一层传播。向前一层传播时，需要考虑到前一个神经元的权重系数(因为不同神经元的重要性不同，因此回传时需要考虑权重系数)。

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/2208992b-f3c5-4a42-8617-edc664a32142.gif)

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/08ba1dd5-49e2-4cb7-b42c-813791a677cc.gif)

例：将误差\delta向\mathrm{f}\_{4}{(e)}传播时，w\_{46}为\mathrm{f}\_{4}{(e)}的权重系数，\mathrm{f}\_{4}{(e)}的误差{\delta }\_{4}=\mathrm{w}\_{46}{\delta }。

与前向传播相同，反向传播后的每一个节点都与前一层的多个节点相连，因此我们在这里求解误差的时候，需要多所有的节点误差求和。我们观察上图中的神经元f\_{1}{(e)},可以发现他同时与f\_{4}{(e)}，f\_{5}{(e)}相连，因此我们在计算f\_{1}{(e)}的误差时需要考虑后一层f\_{4}{(e)},f\_{5}{(e)}的权重系数，因此\delta\_{1}=w\_{14}\delta\_{4}+w\_{15}\delta\_{5}。其他节点采用相同操作。

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/49787723-072a-43d9-b4a5-3818c01f6927.gif)

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/6b5498db-726b-445b-87d9-5fd2d0a54390.gif)

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/8d347e1c-99cd-4db0-b7ef-8bfa6f679967.gif)

#### 2.2.2 更新权重（结合梯度下降食用）

图中的η代表学习率，w'是更新后的权重，通过这个式子来更新权重。这个式子具体是怎么来的，请看下面的具体事例，现在只要先保留大概的印象就行了。

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/d0dca35e-0668-4a2b-a15d-f5bc49125c7d.png)

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/371fdd5e-9057-4d9b-b40f-12fafa464d92.png)

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/a68ab317-c4b9-4a95-a438-b1edce8d3567.png)

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/d6b4e36c-9b88-4981-91a8-2d80df1066ff.png)

#### 2.2.3 具体例子

## 3. 卷积神经网络 (CNN) 

## 4. 循环神经网络 (RNN) 和长短时记忆网络 (LSTM)

- **RNN 基础**
    
- **LSTM 和 GRU**
    

## 5. 注意力机制 (Attention Mechanism)

- **注意力的概念**
    
- **自注意力 (Self-Attention)**
    

## 6. 序列建模

- **编码器-解码器结构**
    

## 7. 位置编码 (Positional Encoding)

## 8. Transformer 的关键组成部分

- **多头注意力机制 (Multi-Head Attention)**
    
- **前馈神经网络 (Feedforward Neural Networks)**
    
- **层归一化 (Layer Normalization)**
    

## 9. 模型的层次结构

- **Encoder 和 Decoder**
    

## 10. 训练技巧

- **学习率调度 (Learning Rate Scheduling)**
    
- **残差连接 (Residual Connections)**
    

## 11. BERT 和 GPT 等预训练模型

- **预训练和微调**
    

## 12. 应用领域

- **自然语言处理 (NLP)**
    
- **图像处理**