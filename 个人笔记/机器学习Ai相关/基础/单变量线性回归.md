是两个变量之间建立的线性方程的拟合模型，用一个变量去预测另一个变量。

**理解：**

数据：

| 面积（m^2） | 价格(万元) |
| ------- | ------ |
| 50      | 85     |
| 70      | 115    |
| 80      | 128    |
| 52      | 66     |
| 62      | 111    |
| 88      | 111    |
| 90      | 142    |
| ...     | ....   |

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/558b3500-c737-4e4d-9585-d56948f67631.png)

现在设想一下 ，如果我们想通过面积去预测最终的价格是否可以建立一个函数用来预测：y=kx+b，为了使函数具有更好的拟合效果 我们加了一个偏置（常数b）

我们改写一下上面函数的形式：h\_{\emptyset }(x)={\emptyset}\_{0}+{\emptyset}\_{1}x,在这里他的函数性质与y=kx+b是一致的。

k**，**{\emptyset}\_{1}**：斜率**

b**，**{\emptyset}\_{0}**：截距**

y**，**h\_{\emptyset }(x)**：预测值**

为了能够成功预测最后的价格我们需要找到一条线段 这条线段能最大程度的拟合这些数据点

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/5e6b1cc3-d75c-4f1d-ab81-e85f0b886978.png)

而我们现在的问题就是这条线该怎么去找？什么样的线才合适？

为了解决这个问题，我们引入了代价函数

### 损失函数（代价函数）

通过对上述的分析，我们可以知道所找的这条线应该接近大部分的样本点，即找到的该直线应该与样本点之间的距离最短，如下：

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/8f0df297-9d4f-4a82-a1a9-8aa503f6b687.png)

为了找一条直线，使得所有样本点与直线之间的距离最小，我们已经知道该直线的模型，即：h\_{\emptyset }(x)={\emptyset}\_{0}+{\emptyset}\_{1}x，那单个样本的误差大小我们就可以表示为d=h\_{\emptyset }(x)-y，那么单个样本的误差公式我们就可以写为：J^{i}(x,y)=h\_{\emptyset }(x)-y那对于所有样本呢？我们采用D= {\textstyle \sum\_{i=1}^{m}}{h\_{\emptyset }({x\_i})-y\_{i}}来表示。这样对吗？思考一下,我们观察图像，对于误差距离是存在正负值的，如果我们只是简单的累加求和是否会存在误差抵消？那有什么办法能够解决这个问题呢？为了解决这个问题,我们对误差取平方再进行除法操作最后的误差公式就变为：J^{i}(x,y)=\frac{1}{2}{(h\_{\emptyset }(x)-y)^2}，

为了更好的评估误差值，一般大家采用平均绝对误差（Mean Absolute Error, MAE）和均方误差（Mean Squared Error, MSE）这两种广泛使用，它们都用于衡量预测值与实际值之间的差异。

#### 均方误差（Mean Squared Error, MSE）

所有预测误差的平方和的平均值\text{MSE} = \frac{1}{n} \sum\_{i=1}^{n} (y\_i - \hat{y}\_i)^2

带入我们的例子：\text{MSE} = {\frac{1}{n}} {{\textstyle \sum\_{i=1}^{m}}{({h\_{\emptyset }({x\_i})}-{y\_{i}}}})^2

#### 平均绝对值误差（Mean Absolute Error MAE）

所有预测误差的绝对值之和的平均值\text{MAE} = \frac{1}{n} \sum\_{i=1}^{n} |y\_i - \hat{y}\_i|

带入我们的例子：\text{MAE} = {\frac{1}{n}} {{\textstyle \sum\_{i=1}^{m}}{|{h\_{\emptyset }({x\_i})}-{y\_{i}}}}|

### 梯度下降

现在我们能够根据损失函数来衡量我们函数的预测值与真实值之间的差距了，那我们需要做的就是如何让预测值与真实值的误差最小。

我们回顾一下我们之前提到的原始函数，以及损失函数：

**原始函数：**h\_{\emptyset }(x)={\emptyset}\_{0}+{\emptyset}\_{1}x 

**损失函数：**J^{i}(x,y)=\frac{1}{2}{(h\_{\emptyset }(x)-y)^2}

在原始函数中我们可以看到两个未知变量（{\emptyset}\_{0},{\emptyset}\_{1}），通过改变这两个变量，都可以达到对误差值最小化。为了能够找到最优解（这里不讨论局部最优解，线性回归函数不存在局部最优），那这里的损失函数也应该是针对这两个变量做误差最小化，那损失函数就可以改写为下面这种形式：

**针对单个数据：**J^{i}({\emptyset}\_{0},{\emptyset}\_{1})=\frac{1}{2}{(h\_{\emptyset }(x)-y)^2}

**推广到整个数据：**J({\emptyset}\_{0},{\emptyset}\_{1}) = {\frac{1}{n}} {{\textstyle \sum\_{i=1}^{m}}{({h\_{\emptyset }({x\_i})}-{y\_{i}}}})^2 

在这里梯度下降的出现是为了将J({\emptyset}\_{0},{\emptyset}\_{1}) = {\frac{1}{n}} {{\textstyle \sum\_{i=1}^{m}}{({h\_{\emptyset }({x\_i})}-{y\_{i}}}})^2 的损失值降到最小。

为了能够理解梯度下降，需要先了解几个前置知识点：**二元函数，偏导数**

#### 二元函数的几何意义

二元函数表示空间直角坐标系中的一个曲面，每一个自变量（x，y）都唯一对应一个因变量z。

比如 z=x^2+y^2,曲面图像如下：

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/8711f446-9181-4e6f-b94f-02fd3f06daed.png)

比如 z=exp(-x^2-y^2)图像如下：

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/cf0db741-b7d4-49b5-8cbb-80c2ee9a4ebd.png)

#### 偏导数的几何意义

对于一元函数的导数我们是知道的，例如：函数y=2x的导数为2,这里的2是函数的斜率。对于二元函数我们同样要研究它的“变化率”。

**几何意义：**

假设有一个多变量函数 f(x\_1, x\_2, \dots, x\_n)，它定义在一个 n维空间中。对于函数 f(x\_1,x\_2)，我们可以把它看作是一个三维的曲面：函数的值f(x1,x2)是平面上的高度。

- **偏导数** \frac{\partial {f}}{\partial{x\_{i}} }代表了函数在某一点\text{比如}(x\_1, x\_2, \dots,x\_{i-1},x\_{i+1}, x\_n)处，沿着x\_i方向的变化率。几何上，这可以理解为该点所在的切平面在 x\_i方向上的斜率。
    

假设我们固定其他变量（比如 x\_1,x\_2,…,x\_i−1,x\_i+1,x\_n），只在x\_i方向上变化。偏导数\frac{\partial {f}}{\partial{x\_{i}} }就表示当我们沿着x\_i方向移动时，函数值的变化速率。

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/eaee1273-8471-4d3e-bb41-f0ffb2a8b878.png)![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/f8d8eb01-6693-4fc9-9b37-6591cd035d12.png)

- **正偏导数**：当某个**偏导数为正时，说明沿着该方向移动，函数值增加。**几何上沿着该方向走会爬**上坡**。
    
- **负偏导数**：当某个**偏导数为负时，说明沿着该方向移动，函数值减小。**几何上沿着该方向走会走**下坡**。
    
- **偏导数为零**：当某个偏导数为零时，说明在该方向上，函数值没有变化。几何上，这时沿着该方向走，曲面不会上升或下降，可能是一个平坦的区域。
    

#### 梯度下降法

|面积（m^2）|价格(万元)|
|---|---|
|50|85|
|70|115|
|80|128|
|52|66|
|62|111|
|88|111|
|90|142|
|...|....|

回到最开始我们的问题上面来，原始函数：h\_{\emptyset }(x)={\emptyset}\_{0}+{\emptyset}\_{1}x <=>y=kx+b,我们为了能够衡量我们的预测值与真实值之间的差距我们引入了损失函数J({\emptyset}\_{0},{\emptyset}\_{1}) = {\frac{1}{n}} {{\textstyle \sum\_{i=1}^{m}}{({h\_{\emptyset }({x\_i})}-{y\_{i}}}})^2，而我们为了能够能够降低损失函数预测值与真实值之间的差距，我们又引入了梯度下降这个概念。

梯度下降的数学公式：\theta\_{n+1}= \theta\_{n}- \eta \cdot \nabla J \left( \theta \right) （{(x\_{n+1},y\_{n+1})}={(x\_n,y\_n)}-{\eta} \cdot { \nabla J ( x,y )}）

\theta\_{n+1}： 下一个值（神经网络中参数更新后的值）

\theta\_{n}：当前值（当前参数值）

\eta：学习率或步长，控制每一步走的距离（需要手动调整的超参数）

\nabla：梯度，函数当前位置的最快上升点（梯度向量指向上坡，负梯度向量指向下坡）

J( \theta )：函数（等待优化的目标函数）

这里我们先假设一个简单一点的损失函数J(x,y)={x}^2+2{y}^2 这个函数形式同样具有两个因变量（**在这里**x,y**都表示为误差值不要与原始函数混淆**），我们通过这个函数我们来描述梯度下降的优化过程。

首先我们先求解函数的梯度（对每一项参数求偏导）

\frac{ \partial J (x ,y )}{ \partial x}=2x 

\frac{ \partial J ( x,y )}{ \partial y}=4y 

然后我们带入参数值：这边先假设参数值（\theta\_{n}）为：a(3,3) ,学习率（\eta）为：0.1

\begin{align} {(x\_{n+1},y\_{n+1})}={(x\_n,y\_n)}-{\eta} \cdot { \nabla J ( x,y )} \\ ={(x\_n,y\_n)}-{\eta} \cdot { (2x,4y)} \end{align}

##### 计算过程

**第一次迭代：**(x\_{n+1},y\_{n+1})  =(-3,-3)-0.1(2\*-3,4\*-3)  =(-3+0.6,-3+1.2) =(-2.4,-1.8)

将值带入损失函数J(x,y)={x}^2+2{y}^2中 J(x,y)={(-2.4)}^2+2\*({-1.8})^2=12.24

**第二次迭代：**

(x\_{n+1},y\_{n+1})  =(-2.4,-1.8)-0.1(2\*-2.4,4\*-1.8)  =(-1.92,-1.08)

将值带入损失函数中J(x,y)={(-1.92)}^2+2\*({-1.08})^2=6.0192

经过不断的迭代，我们可以发现误差在不断的减小。

##### 梯度下降核心思想

- 计算损失函数对参数的梯度（即偏导数）。
    
- 按照梯度的反方向调整参数，以减少损失函数的值。
    
- 通过多次迭代更新，模型的参数会逐渐趋向最优解。
    

![v2-451f15e18a969fc1a4c64707205832ec_r.jpeg](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/1wvqr7pgAwMQOako/img/612335e7-92c5-44fa-b1ab-746a608fc685.jpeg)

#### 批量梯度下降法（BGD，Batch Gradient Descent）

回顾之前所讲的梯度下降法公式：\theta\_{n+1}= \theta\_{n}- \eta \cdot \nabla J \left( \theta \right) ，而所谓批量梯度下降（BGD，Batch Gradient Descent）它的具体思路是在更新每一参数时都使用所有的样本来进行梯度的更新。我们将我们上面的公式推广到多个样本中:

\theta\_{n+1}= \theta\_{n}- \eta\cdot \frac{1}{m}  \sum\_{i=1}^{n} \nabla J({\theta\_n};X^{i},Y^{i} )

m 是训练集的样本数量。

X^{(i)} 和 Y^{(i)}分别是第 i个样本的输入和标签。

∇J(θ\_n;X^{(i)},Y^{(i)}) 是损失函数 J(\theta) 对于每个样本的梯度。

这里我们假设损失函数 J(\theta)是基于均方误差（MSE）的损失函数，那么对参数\theta的梯度通常是：

\nabla {J}(\theta )= {\frac{1}{m}}{\sum\_{i=1}^{m}(H\_\theta(X^{(i)})-Y^{(i)})}X^{(i)}

H\_θ(X^{(i)}) ：模型对于第i个样本的预测

Y^{(i)}：第i个训练样本的真实标签

X^{(i)}：第i个训练样本的特征向量

结合起来，批量梯度下降的更新规则就是：

\theta\_{n+1}= \theta\_{n}-{\eta} {\cdot}\frac{1}{m} \sum\_{i=1}^{m}  ({H\_{\theta}}( X^{(i)})-Y^{(i)}) {x}^{(i)}

这就是基于整个数据集计算的梯度下降更新规则，目标是通过所有样本的信息来更新参数\theta。**批量梯度下降通过使用整个数据集的梯度信息来更准确地更新参数，因此相较于SGD，收敛较为平稳，但每次更新的计算开销较大，通常需要更多的计算资源和时间。**

#### 随机梯度下降法（SGD，Stochastic Gradient Descent）

随机梯度下降法（SGD，Stochastic Gradient Descent）是一种基于单个样本进行参数更新的优化方法，与批量梯度下降（Batch Gradient Descent）相比，SGD在每次迭代中仅计算一个样本的梯度，从而使得每次参数更新的计算量较小，且可以更快地迭代。但是，由于其更新基于单个样本，收敛过程通常会比批量梯度下降更加波动。

在梯度下降的背景下，使用样本(X^{(i)}, Y^{(i)}) 的梯度来更新参数的方式就是 **随机梯度下降**，它的更新公式是：

\theta\_{n+1}= \theta\_{n}- \eta\cdot  \nabla J({\theta\_n};X^{i},Y^{i} )

其中\theta\_n 是第n次迭代的参数，\eta是学习率，\nabla J(\theta\_n; X^{(i)}, Y^{(i)})是基于第i个样本的损失函数的梯度。

在这里我们同样假设损失函数 J(\theta)是基于均方误差（MSE）的损失函数

J(\theta) = \frac{1}{2m} \sum\_{i=1}^{m} (H\_{\theta}(X^{(i)}) - Y^{(i)})^2

其中，H\_{\theta}(X^{(i)})是模型的预测，Y^{(i)} 是真实值，m 是样本数。对于一个样本(X^{(i)}, Y^{(i)})，损失函数关于参数 \theta\_j的梯度为：\nabla J(\theta\_n; X^{(i)}, Y^{(i)}) = (H\_{\theta}(X^{(i)}) - Y^{(i)}) \cdot X^{(i)}\_j

因此，SGD 的更新公式就变成：\theta\_j = \theta\_j - \eta \cdot (H\_{\theta}(X^{(i)}) - Y^{(i)}) \cdot X^{(i)}\_j

##### 迭代过程

1. **随机选择一个样本**：在每次迭代中，从数据集中随机选择一个样本 (X^{(i)}, Y^{(i)})。
    
2. **计算梯度**：基于当前参数\theta\_n 和所选样本((X^{(i)}, Y^{(i)})，计算损失函数的梯度\nabla J(\theta\_n; X^{(i)}, Y^{(i)})
    
3. **更新参数**：使用计算得到的梯度和学习率 \eta 更新参数 \theta\_j。
    

这种方式在每次迭代时，只考虑单个样本的数据，避免了批量梯度下降所需要的庞大计算量。尽管SGD的更新可能较为震荡，但它有较好的表现，尤其在数据量很大的时候。

##### 随机梯度下降的特点

- **单个样本更新**：与批量梯度下降不同，SGD 在每次更新中只用一个样本 (X^{(i)}, Y^{(i)})来计算梯度并更新参数，而不是使用所有样本。
    
- **噪声和波动**：由于每次更新都基于一个样本的梯度，SGD 在每次迭代时的更新方向可能会有所不同，从而导致参数更新的路径呈现出更多的波动。虽然这种波动可能让收敛过程变得不稳定，但它也使得SGD能够逃脱局部最优解，并能在大规模数据集上快速收敛。
    
- **计算效率**：由于每次迭代只考虑一个样本，因此每次更新所需的计算量非常小，SGD适用于数据量非常大的问题。
    

#### 小批量随机梯度下降（MBGD）（略）

### 总结

在单变量线性回归当中，我们为了能够通过面积去预测价格，我们通过因变量(y)与自变量(x)建立了一个在线性关系的函数：h\_{\emptyset }(x)={\emptyset}\_{0}+{\emptyset}\_{1}x,为了能够更好的衡量我们函数预测值与真实数据的一个误差，我们又引入了损失函数：J({\emptyset}\_{0},{\emptyset}\_{1}) = {\frac{1}{n}} {{\textstyle \sum\_{i=1}^{m}}{({h\_{\emptyset }({x\_i})}-{y\_{i}}}})^2,最后我们为了能够将预测值与真实值之间的误差减小，我们使用了梯度下降算法以及学习率这个概念：\theta\_{n+1}= \theta\_{n}- \eta \cdot \nabla J \left( \theta \right)。我们通过对权重，偏置求偏导，从而不断的减少误差值达到一个最佳的值。