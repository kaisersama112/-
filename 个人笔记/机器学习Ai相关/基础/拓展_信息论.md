## 信息论基础

​	信息论（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。

### 1. 熵

​	在引入熵的概念之前，我们思考一个问题：

​	```抛一枚有均匀正反面的硬币，和掷一个均匀六面的骰子，哪一种试验的不确定性更强一点呢？```

​	粗略地看，我们感觉抛硬币这个试验的不确定性会更少一点，因为硬币毕竟仅有2个结果，而骰子有6个结果，但是对于这样一个直觉的事实，我们怎么进行量化从而在数字层面上反映两个随机变量的不确定性的大小关系呢？

​	Shannon提出了熵的概念，解决了以上这个问题。对于上述离散型随机事件，可以用离散熵定义其不确定性：

​		![image-20240411171953395](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240411171953395.png)

​	下面用Shannon离散熵量化解决我们之前引入的两个试验：

​		![image-20240411172017302](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240411172017302.png)

​	我们可以更进一步地看，**一个随机变量的熵越大，意味着不确定性越大，那么也就是说，该随机变量包含的信息量越大**，那到底信息量是什么呢？抛一枚硬币的信息量就是，正面朝上，反面朝上，这2就是信息量；同样，掷骰子的信息量就是6个不同数字的面朝上，这6也是信息量。那么，在计算机角度上看，熵到底是什么，我们不妨看![image-20240411172047929](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240411172047929.png)

​	其实就是意味着，在计算机中，要表示抛硬币的结果，需要用1 bit，要表示掷骰子的结果需要用log6 bit（实际表示时为向上取整3 bit），也就是说**熵是平均意义上对随机变量的编码长度。**为什么这么说呢？

![image-20240411172114042](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240411172114042.png)

​	现在我们了解到熵在信息论中的意义和以及在计算机编码中的物理含义。最后我们再说明一下，必然事件的熵是多少呢？应该是0，因为必然是事件是确定无疑的，并不含有不确定性，也就是说，必然事件不含有信息量。

​	信息论的核心思想是量化数据中的信息内容。在信息论中，该数值被称为分布P的熵（entropy）。可以通过 以下方程得到：

​	![image-20240411171316434](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240411171316434.png)

​	信息论的基本定理之一指出，为了对从分布p中随机抽取的数据进行编码，我们至少需要H[P]“纳特（nat）” 对其进行编码。“纳特”相当于比特（bit），但是对数底为e而不是2。因此，一个纳特是 1/ log(2) ≈ 1.44比特。

### 2. 互信息

​	熵表明了单个随机变量的不确定程度，那么熵的值是确定不变的吗？我们有办法缩减这个不确定性吗？如果能缩减那缩减多少可以量化吗？

​	我们举一个例子来说明事件不确定性的变化：

​	```假设现在我给你一枚硬币，告诉你这是均匀的，请你抛100次然后告诉我结果，结果你抛了100次后，记录的结果是：正面朝上90次，反面朝上10次，你就会开始怀疑“这真是一枚均匀的硬币吗？”```

​	在第一部分熵中，我们知道，这一枚硬币的熵应该是1 bit，但是这样的试验之后，这枚硬币的熵还是1 bit吗？我们可以假设正面朝上的概率为0.9，反面朝上的概率为0.1，计算一下这个熵：![image-20240411172457610](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240411172457610.png)

​	其中，H（X|X^）表示为知道90次正面朝上的事实后，原硬币的熵。

​	经过抛掷100次后，我们知道这么硬币可能是不均匀的，且新的熵为0.469 bit，也就是说我们在知道90次正面朝上，10次反面朝下的事实之后，这个硬币的熵缩小了0.531 bit，这个0.531的信息量，我们就称为**互信息**。

​	现在我们引入互信息的定义：![image-20240411172613699](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240411172613699.png)

​	推导如下：![image-20240411172640548](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240411172640548.png)

​	我们可以看出H（X）表示为原随机变量X的信息量，H（X|Y）为知道事实后的信息量，互信息量I(X|X)则表示为知道事实Y后，原来信息量减少了多少。

​	![image-20240411172840196](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240411172840196.png)



### 3. 信息量

​	压缩与预测有什么关系呢？想象一下，我们有一个要压缩的数据流。如果我们很容易预测下一个数据，那么 这个数据就很容易压缩。为什么呢？举一个极端的例子，假如数据流中的每个数据完全相同，这会是一个非 常无聊的数据流。由于它们总是相同的，我们总是知道下一个数据是什么。所以，为了传递数据流的内容，我 们不必传输任何信息。也就是说，“下一个数据是xx”这个事件毫无信息量。 但是，如果我们不能完全预测每一个事件，那么我们有时可能会感到“惊异”。**克劳德·香农**决定用信息 量log 1/P (j) = − log P(j)来量化这种惊异程度。在观察一个事件j时，并赋予它（主观）概率P(j)。当我们赋 予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。熵：当分配的概率真正匹配数据生成过程时的信息量的期望。

### 4. 交叉熵

​	如果把熵H(P)想象为“知道真实概率的人所经历的惊异程度”，那么什么是交叉熵？交叉熵从P到Q，记 为H(P, Q)。我们可以把交叉熵想象为“主观概率为Q的观察者在看到根据概率P生成的数据时的预期惊异”。 当P = Q时，交叉熵达到最低。在这种情况下，从P到Q的交叉熵是H(P, P) = H(P)。 简而言之，我们可以从两方面来考虑交叉熵分类目标：

（i）最大化观测数据的似然；

（ii）最小化传达标签所 需的惊异。

### 5. 相对熵

​	对于孤立的一个随机变量我们可以用熵来量化，对于两个随机变量有依赖关系，我们可以用互信息来量化，那么对于两个随机变量之间相差多少？也就是说，这两个随机变量的分布函数相似吗？如果不相似，那么它们之间差可以量化吗？

​	相对熵它给出了两个分布之间的差异程度的量化，也就说相对熵代表的是这个两个分布的“距离”。

​	相对熵的定义：![image-20240411173007825](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240411173007825.png)

​	相对熵应用场景，假设某一个样本服从分布 p(x)，我们通过样本拟合出来的分布是 q(x) ,那么他们之间的差异程度就是 D(p||q) ，再一步而言， 根据分布 p(x) 我们得出表示随机变量的码长为其熵 H(p) （该表达等价于 H(X) ），而我们估计的分布为 q(x) ,那么他们的关系可以表达为：![image-20240411173126440](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240411173126440.png)

