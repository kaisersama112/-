

## 多层感知机（MLP）

​	之前我们实现了softmax回归，在这个过程中，我们学习率如何处理数据，如何将输出转换为有效的概率分布，并应用适当的损失函数，根据模型参数最小化损失。我们已经在简单的线性模型背景下掌握了这些知识，现在我们可以开始对深度神经网络的探索
​	在softmax回归的模型架构。我们描述了仿射变换，它是一种带有偏置项的线性变换。该模型通过单个仿射变换将我们的输入直接映射到输出，然后进行softmax操作。如果我们的标签通过仿射变换后确实与我们的输入数据相关，那么这种方法确实足够了。但是，仿射变换中的线性是一个很强的假设。

​	这意味着线性模型可能会出错，例如，线性意味着单调假设：任何特征的增大都会导致模型输出的增大（如果对应的权重为正），或者导致模型输出的减小（如果对应的权重为负）。

​	然而我们可以很容易找出违反单调性的例子。例如，我们想要根据体温预测死亡率。对体温高于37摄氏度的人来说，温度越高风险越大。然而，对体温低于37摄氏度的人来说，温度越高风险就越低。在这种情况下，我们也可以通过一些巧妙的预处理来解决问题。例如，我们可以使用与37摄氏度的距离作为特征。

​	但是，如何对猫和狗的图像进行分类呢？增加位置(13, 17)处像素的强度是否总是增加（或降低）图像描绘狗的似然？对线性模型的依赖对应于一个隐含的假设，即区分猫和狗的唯一要求是评估单个像素的强度。在一
个倒置图像后依然保留类别的世界里，这种方法注定会失败。

​	与前面的例子相比，这里的线性很荒谬，而且我们难以通过简单的预处理来解决这个问题。这是因为任何像素的重要性都以复杂的方式取决于该像素的上下文（周围像素的值）。我们的数据可能会有一种表示，这种表示会考虑到我们在特征之间的相关交互作用。在此表示的基础上建立一个线性模型可能会是合适的，但
​	我们不知道如何手动计算这么一种表示。对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。

### 1. 隐藏层

​	我们通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。要做到这一点，最简单的方法是将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。

​	我们可以把前L−1层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。下面，我们以图的方式描述了多层感知机
![Snipaste_2024-04-12_15-13-16](https://raw.githubusercontent.com/kaisersama112/typora_image/master/Snipaste_2024-04-12_15-13-16.png)
​	这个图像描述了一个单隐藏层的多层感知机，该模型具有五个隐藏单元这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。

​	输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。因此，这个多层感知机中的层数为2。注意，这两个层都是全连接的。每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。然而，正如之前所说，具有全连接层的多层感知机的参数开销可能会高得令人望而却步。



### 2. 从线性到非线性

​	同之前的章节一样，我们通过矩阵X ∈ R^(n×d )来表示n个样本的小批量，其中每个样本具有d个输入特征。 对于具有h个隐藏单元的单隐藏层多层感知机，用H ∈ R^(n×h)表示隐藏层的输出，称为隐藏表示（hidden representations）。

​	在数学或代码中，H也被称为隐藏层变量（hidden‐layer variable）或隐藏变量（hidden variable）。因为隐藏层和输出层都是全连接的，所以我们有隐藏层权重W(1) ∈ R^(d×h) 和隐藏层偏置b (1) ∈ R^(1×h) 以及输出层权重W(2) ∈ R^(h×q) 和输出层偏置b (2) ∈ R^(1×q)。形式上，我们按如下方式计算单隐藏层多层感知机 的输出 O ∈ R^(n×q)：

![image-20240412153013342](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240412153013342.png)

​	注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。可我们能从中得到什么好处呢？在上面定义 的模型里，我们没有好处！原因很简单：上面的隐藏单元由输入的仿射函数给出，而输出（softmax操作前） 只是隐藏单元的仿射函数。仿射函数的仿射函数本身就是仿射函数，但是我们之前的线性模型已经能够表示 任何仿射函数。

​	我们可以证明这一等价性，即对于任意权重值，我们只需合并隐藏层，便可产生具有参数 `W = W(1)W(2) 和b = b (1)W(2) + b (2)` 的等价单层模型：

​	![image-20240412160426673](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240412160426673.png)

​	为了发挥多层架构的潜力，我们还需要一个额外的关键要素：在仿射变换之后对每个隐藏单元应用非线性的 激活函数（activation function）σ。激活函数的输出（例如，σ(·)）被称为活性值（activations）。一般来说， 有了激活函数，就不可能再将我们的多层感知机退化成线性模型：![image-20240412160754460](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240412160754460.png)

​	由于X中的每一行对应于小批量中的一个样本，出于记号习惯的考量，我们定义非线性函数σ也以按行的方 式作用于其输入，即一次计算一个样本。我们在 3.4.5节中以相同的方式使用了softmax符号来表示按行操作。 

​	本节应用于隐藏层的激活函数通常不仅按行操作，也按元素操作。这意味着在计算每一层的线性部分之 后，我们可以计算每个活性值，而不需要查看其他隐藏单元所取的值。

​	对于大多数激活函数都是这样。 为了构建更通用的多层感知机，我们可以继续堆叠这样的隐藏层，例如H(1) = σ1(XW(1) + b (1))和H(2) = σ2(H(1)W(2) + b (2))，一层叠一层，从而产生更有表达能力的模型。



### 3. 通用近似定理

​	多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用，这些神经元依赖于每个输入的值。我们可以很容易地设计隐藏节点来执行任意计算。

​	例如，在一对输入上进行基本逻辑操作，多层感知机是通用近似器。即使是网络只有一个隐藏层，给定足够的神经元和正确的权重，我们可以对任意函数建模，尽管实际中学习该函数是很困难的。神经网络有点像C语言。C语言和任何其他现代编程语言一样，能够表达任何可计 算的程序。但实际上，想出一个符合规范的程序才是最困难的部分。

​	而且，虽然一个单隐层网络能学习任何函数，但并不意味着我们应该尝试使用单隐藏层网络来解决所有问题。 事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。

### 4. 激活函数
激活函数（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号 转换为输出的可微运算。大多数激活函数都是非线性的。由于激活函数是深度学习的基础，下面简要介绍一 些常见的激活函数。
####  - ReLU函数

​	最受欢迎的激活函数是修正线性单元（Rectified linear unit，ReLU），它实现简单，同时在各种预测任务中表现良好。ReLU提供了一种非常简单的非线性变换。给定元素x，ReLU函数被定义为该元素与0的最大值：该函数定义为![image_Snipaste_2024-01-29_14-55-30.png](https://raw.githubusercontent.com/kaisersama112/typora_image/master/assetsSnipaste_2024-01-29_14-55-30.png)

```python

def detach_plot(x, y, x_label, y_label, title):
    plt.plot(x.numpy(), y.numpy(), label=title)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.title(title)
    plt.grid(True)
    plt.legend()
    plt.show()


x = torch.range(-8.0, 8, 0.1, requires_grad=True)
y = torch.relu(x)
detach_plot(x.detach(), y.detach(), 'x', 'rule(x)', 'ReLU Function')

```

![output](https://raw.githubusercontent.com/kaisersama112/typora_image/master/output.png)

​	通过观察上面的函数图像，显然，当输入为负数时，ReLU函数的导数为0；当输入为正数时，ReLU函数的导数为1。尽管输入为0时ReLU函数不可导，但是我们可以取此处的导数为0。下面绘制ReLU函数的导数。

```python

y.backward(torch.ones_like(x), retain_graph=True)

detach_plot(x.detach(), x.grad, 'x', 'grad of relu', 'ReLU backward')
```



![rule_backward](https://raw.githubusercontent.com/kaisersama112/typora_image/master/rule_backward.png)

​	使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题

​	注意，ReLU函数有许多变体，包括参数化ReLU（Parameterized ReLU，pReLU）函数。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：

![image-20240412163356222](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240412163356222.png)



#### - sigmoid函数
​	对于一个定义域在R中的输入，sigmoid函数将输入变换为区间(0, 1)上的输出。因此，sigmoid通常称为挤压函数（squashing function）：它将范围（‐inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：
![image_Snipaste_2024-01-29_14-57-09.png](https://raw.githubusercontent.com/kaisersama112/typora_image/master/assetsSnipaste_2024-01-29_14-57-09.png)
​	它是一个平滑的、可微的阈值单元近似。当我们想要将输出视作二元分类问题的概率时，sigmoid仍然被广泛用作输出单元上的激活函数（sigmoid可以视为softmax的特例)。

​	sigmoid函数在早期的神经网络中较为普遍，但它目前逐渐被更简单的ReLU函数取代。在后面“循环神经网络”一章中我们会介绍如何利用它值域在0到1之间这一特性来控制信息在神经网络中的流动。下面绘制了sigmoid函数。当输入接近0时，sigmoid函数接近线性变换。

```python
y = torch.sigmoid(x)
detach_plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', 'sigmoid function')
```

![sigmoid_function](https://raw.githubusercontent.com/kaisersama112/typora_image/master/sigmoid_function.png)

依据链式法则，sigmoid函数的导数为:![image_Snipaste_2024-01-29_14-58-10.png](https://raw.githubusercontent.com/kaisersama112/typora_image/master/assetsSnipaste_2024-01-29_14-58-10.png)

​	当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0。

```python
x.grad.zero_()
y.backward(torch.ones_like(x), retain_graph=True)
detach_plot(x.detach(), x.grad, 'x', 'sigmoid(x)', 'sigmoid backward')
```

![sigmoid_backward](https://raw.githubusercontent.com/kaisersama112/typora_image/master/sigmoid_backward.png)



#### - tanh函数
与sigmoid函数类似，tanh(双曲正切)函数也能将其输入压缩转换到区间(‐1, 1)上。tanh函数的公式如下：
![image_Snipaste_2024-01-29_14-59-11.png](https://raw.githubusercontent.com/kaisersama112/typora_image/master/assetsSnipaste_2024-01-29_14-59-11.png)
	我们接着绘制tanh函数。当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。

```python
y = torch.tanh(x)
detach_plot(x.detach(), y.detach(), 'x', 'tanh(x)', 'tanh function')
```

![tanh_function](https://raw.githubusercontent.com/kaisersama112/typora_image/master/tanh_function.png)

依据链式法则，tanh函数的导数为
![imge_Snipaste_2024-01-29_14-59-56.png](https://raw.githubusercontent.com/kaisersama112/typora_image/master/assetsSnipaste_2024-01-29_14-59-56.png)

​	tanh函数的导数图像如下所示。当输入接近0时，tanh函数的导数接近最大值1。与我们在sigmoid函数图像 中看到的类似，输入在任一方向上越远离0点，导数越接近0。

```python
x.grad.zero_()
y.backward(torch.ones_like(x), retain_graph=True)
detach_plot(x.detach(), x.grad, 'x', 'tanh(x)', 'tanh backward')
```

![tanh_backward](https://raw.githubusercontent.com/kaisersama112/typora_image/master/tanh_backward.png)



### 5. 多层感知机实现

​	这里我们继续采用与softmax回归相同的数据集（用Fashion‐MNIST图像分类数据集）

```python
import torch
from torch import nn
from d2l import torch as d2l
```

```python
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)
```

#### - 初始化模型参数

​	Fashion‐MNIST中的每个图像由 28 × 28 = 784个灰度像素值组成。所有图像共分为10个类别。忽略像素之间的空间结构，我们可以将每个图像视为具有784个输入特征和10个类的简单分类数据集。首先，我 们将实现一个具有单隐藏层的多层感知机，它包含256个隐藏单元。注意，我们可以将这两个变量都视为超参数。

​	通常，我们选择2的若干次幂作为层的宽度。因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。 我们用几个张量来表示我们的参数。注意，对于每一层我们都要记录一个权重矩阵和一个偏置向量。跟以前 一样，我们要为损失关于这些参数的梯度分配内存。

```python
import torch
from torch import nn
from d2l import torch as d2l
from IPython import display

```

```python
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)
```

```python
num_inputs, num_outputs, num_hidden = 784, 10, 256

w1 = nn.Parameter(torch.randn(num_inputs, num_hidden, requires_grad=True) * 0.01)
print(f'w1 shape: {w1.shape}')
b1 = nn.Parameter(torch.zeros(num_hidden, requires_grad=True))
print(f'b1 shape: {b1.shape}')
w2 = nn.Parameter(torch.randn(num_hidden, num_outputs, requires_grad=True) * 0.01)
print(f'w2 shape: {w2.shape}')
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))
print(f'b2 shape: {b2.shape}')
params = [w1, b1, w2, b2]
```

输入：

```cmd
w1 shape: torch.Size([784, 256])
b1 shape: torch.Size([256])
w2 shape: torch.Size([256, 10])
b2 shape: torch.Size([10])
```

#### - 激活函数

```python
# 激活函数
def relu(x):
    a = torch.zeros_like(x)
    return torch.max(x, a)
```

####  -模型

```python
# 模型
def net(x):
    x = x.reshape((-1, num_inputs))
    # @ 在这里表示矩阵乘法
    h = relu(x @ w1 + b1)
    g = (h @ w2 + b2)
    return g
```



#### - 损失函数

```python
# 损失函数(交叉熵损失)
loss = nn.CrossEntropyLoss(reduction='none')
```



#### - 训练

```python
import torch

import d2l.torch as d2l
from IPython import display as soft_max_display


class Accumulator: 
    """在n个变量上累加"""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]


class Animator:
    """在动画中绘制数据"""

    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # 增量地绘制多条线
        if legend is None:
            legend = []
        d2l.use_svg_display()
        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # 使用lambda函数捕获参数
        self.config_axes = lambda: d2l.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        # 向图表中添加多个数据点
        if not hasattr(y, "__len__"):
            y = [y]

        n = len(y)
        if not hasattr(x, "__len__"):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        soft_max_display.display(self.fig)
        soft_max_display.clear_output(wait=True)


def accuracy(y_hat, y):
    """
    模型预测准确率
    """
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    cmp_type = cmp.type(y.dtype)
    return float(cmp_type.sum())


def evaluate_accuracy(net, data_iter):
    """计算在指定数据集上模型的精度"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # 将模型设置为评估模式
    metric = Accumulator(2)
    with torch.no_grad():
        for x, y in data_iter:
            metric.add(accuracy(net(x), y), y.numel())
    return metric[0] / metric[1]


def train_epoch(net, train_iter, loss, updater):
    """

    """
    if isinstance(net, torch.nn.Module):
        net.train()
    metric = Accumulator(3)
    for x, y in train_iter:
        y_hat = net(x)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            l.sum().backward()
            updater(x.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    print(f'metric:{metric.data}')
    return metric[0] / metric[2], metric[1] / metric[2]


def train(net, train_iter, test_iter, loss, num_epochs, updater):
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])

    for epoch in range(num_epochs):
        train_metrics = train_epoch(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        print(train_metrics)
        print(test_acc)
        animator.add(epoch + 1, train_metrics + (test_acc,))

    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc


def predict(net, test_iter, n=6):
    for X, y in test_iter:
        print(X.shape)
        print(y.shape)
        break

    trues = d2l.get_fashion_mnist_labels(y)
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
    titles = [true + '\n' + pred for true, pred in zip(trues, preds)]
    d2l.show_images(
        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])

```



```python
# 这里的训练与之前的softmax相同 这边直接拿过来使用就好了
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
from soft_max_utils import train
train(net=net, train_iter=train_iter, test_iter=test_iter, loss=loss, num_epochs=num_epochs, updater=updater)
```

![image-20240412175237688](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240412175237688.png)

#### - 预测

```python
from soft_max_utils import predict
predict(net, test_iter)
```

![image-20240412175244038](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240412175244038.png)

#### - 使用torch 高级api实现

```python
# 使用pytorch api实现


import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10)
                    )


def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)


net.apply(init_weights)

batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=lr)
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)
from soft_max_utils import train

train(net=net, train_iter=train_iter, test_iter=test_iter, loss=loss, num_epochs=num_epochs, updater=trainer)

```

![image-20240416160929899](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240416160929899.png)

### 6. 模型选择，欠拟合和过拟合

​	我们将模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting），用于对抗过拟合的技术 称为正则化（regularization）。

#### - 训练误差和泛化误差

​	训练误差（training error）是指模型在练数据集上计算得到的误差。

​	泛化误差（generalization error）是指模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。

​	问题是，我们永远不能准确地计算出泛化误差。这是因为无限多的数据样本是一个虚构的对象。在实际中，我 们只能通过将模型应用于一个独立的测试集来估计泛化误差，该测试集由随机选取的、未曾在训练集中出现 的数据样本构成。
