# 深度学习计算

​	在这里我们将深入讨论深度学习计算的关键组件，即模型构建、参数访问与初始化、设计自定义层和块、将模型读写到磁盘，以及利用GPU实现加速。

## 1. 层与块

​		之前在学习神经网络时，我们关注的是具有单一输出的线性模型。在这里，整个模型只有一个输出。注意， 单个神经网络

​		（1）接受一些输入；

​		（2）生成相应的标量输出；

​		（3）具有一组相关 参数（parameters），更新 这些参数可以优化某目标函数。 

​		然后，当考虑具有多个输出的网络时，我们利用矢量化算法来描述整层神经元。像单个神经元一样，层（1） 接受一组输入，（2）生成相应的输出，（3）由一组可调整参数描述。当我们使用softmax回归时，一个单层本身就是模型。然而，即使我们随后引入了多层感知机，我们仍然可以认为该模型保留了上面所说的基本架构。 

​		对于多层感知机而言，整个模型及其组成层都是这种架构。整个模型接受原始输入（特征），生成输出（预 191 测），并包含一些参数（所有组成层的参数集合）。同样，每个单独的层接收输入（由前一层提供），生成输出 （到下一层的输入），并且具有一组可调参数，这些参数根据从下一层反向传播的信号进行更新。 

​		事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值。例如，在计算机视觉中广泛流行 的ResNet‐152架构就有数百层，这些层是由层组（groups of layers）的重复模式组成。这个ResNet架构赢得 了2015年ImageNet和COCO计算机视觉比赛的识别和检测任务 (He et al., 2016)。目前ResNet架构仍然是许多 视觉任务的首选架构。在其他的领域，如自然语言处理和语音，层组以各种重复模式排列的类似架构现在也 是普遍存在。 为了实现这些复杂的网络，我们引入了神经网络块的概念。块（block）可以描述单个层、由多个层组成的组 件或整个模型本身。使用块进行抽象的一个好处是可以将一些块组合成更大的组件，这一过程通常是递归的， 如 图1所示。通过定义代码来按需生成任意复杂度的块，我们可以通过简洁的代码实现复杂的神经网络。

> 图1: 多个层被组合成块，形成更大的模型
>
> ![image-20240514150451086](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240514150451086.png)

​		从编程的角度来看，块由类（class）表示。它的任何子类都必须定义一个将其输入转换为输出的前向传播函 数，并且必须存储任何必需的参数。注意，有些块不需要任何参数。最后，为了计算梯度，块必须具有反向传播函数。在定义我们自己的块时，由于自动微分提供了一些后端实现，我们只需要考虑前向传播函数和必需的参数。

​		 在构造自定义块之前，我们先回顾一下多层感知机的代码。下面的代码生成一个网络，其中包含一 个具有256个单元和ReLU激活函数的全连接隐藏层，然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。

```python
import torch
from torch import nn
from torch.nn import functional as F
net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
X = torch.rand(2, 20)
net(X)
```

```cmd
tensor([[ 0.0343, 0.0264, 0.2505, -0.0243, 0.0945, 0.0012, -0.0141, 0.0666,
-0.0547, -0.0667],
[ 0.0772, -0.0274, 0.2638, -0.0191, 0.0394, -0.0324, 0.0102, 0.0707,
-0.1481, -0.1031]], grad_fn=<AddmmBackward0>)
```

### 1.1 自定义块

​		要想直观地了解块是如何工作的，最简单的方法就是自己实现一个。在实现我们自定义块之前，我们简要总 结一下每个块必须提供的基本功能。 

  		1.  将输入数据作为其前向传播函数的参数。 
  		1.  通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的 第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。 
  		1.  计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。 
  		1.  存储和访问前向传播计算所需的参数。
  		1.  根据需要初始化模型参数。 

​		在下面的代码片段中，我们从零开始编写一个块。它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一 个10维输出层。注意，下面的MLP类继承了表示块的类。我们的实现只需要提供我们自己的构造函数（Python中 的_\_init\_\_函数）和前向传播函数。

```python
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, x):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(x)))
```

​		我们首先看一下前向传播函数，它以X作为输入，计算带有激活函数的隐藏表示，并输出其未规范化的输出 值。在这个MLP实现中，两个层都是实例变量。要了解这为什么是合理的，可以想象实例化两个多层感知机 （net1和net2），并根据不同的数据对它们进行训练。当然，我们希望它们学到两种不同的模型。 

​		接着我们实例化多层感知机的层，然后在每次调用前向传播函数时调用这些层。注意一些关键细节：首先， 我们定制的\_\_init\_\_函数通过super()._\_init\_\_() 调用父类的\_\_init\_\_函数，省去了重复编写模版代码的痛 苦。然后，我们实例化两个全连接层，分别为self.hidden和self.out。注意，除非我们实现一个新的运算符， 否则我们不必担心反向传播函数或参数初始化，系统将自动生成这些。

```python
net = MLP()
X = torch.rand(2, 20)
net(X)
```

```cmd
tensor([[-0.0604,  0.0864, -0.0428, -0.0479, -0.0406, -0.1178, -0.1485, -0.0185,
          0.1243,  0.0782],
        [-0.0237,  0.0241, -0.0393,  0.0787, -0.1034, -0.1370, -0.1549,  0.0262,
          0.1562,  0.1874]], grad_fn=<AddmmBackward0>)
```

块的一个主要优点是它的多功能性。我们可以子类化块以创建层（如全连接层的类）、整个模型（如上面 的MLP类）或具有中等复杂度的各种组件。我们在接下来的章节中充分利用了这种多功能性，比如在处理卷积 神经网络时。

### 1.2 顺序块

​	现在我们可以更仔细地看看Sequential类是如何工作的，回想一下Sequential的设计是为了把其他模块串起 来。为了构建我们自己的简化的MySequential，我们只需要定义两个关键函数：

  	 1.  一种将块逐个追加到列表中的函数；
  	 2.  一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。 

下面的MySequential类提供了与默认Sequential类相同的功能。

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。_module的类型是OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X
```

​		\_\_init\_\_函数将每个模块逐个添加到有序字典_modules中。为什么每个Module都有一 个\_modules属性？为什么我们使用它而不是自己定义一个Python列表？简而言之，\_modules的主要优 点是：在模块的参数初始化过程中，系统知道在_modules字典中查找需要初始化参数的子块。

​		当MySequential的前向传播函数被调用时，每个添加的块都按照它们被添加的顺序执行。现在可以使用我们 的MySequential类重新实现多层感知机。

```python
net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
X = torch.rand(2, 20)
net(X)
```

```cmd
tensor([[-0.1874,  0.0318,  0.3143,  0.0213,  0.0829,  0.3726,  0.3041,  0.1828,
         -0.0331,  0.1403],
        [-0.1743, -0.0048,  0.2559,  0.0536,  0.1307,  0.2933,  0.1923,  0.1516,
         -0.1273,  0.0501]], grad_fn=<AddmmBackward0>)
```

MySequential的用法与之前为Sequential类编写的代码相同。

### 1.3 在前向传播函数中执行代码

​		Sequential类使模型构造变得简单，允许我们组合新的架构，而不必定义自己的类。然而，并不是所有的架 构都是简单的顺序架构。当需要更强的灵活性时，我们需要定义自己的块。例如，我们可能希望在前向传播 函数中执行Python的控制流。此外，我们可能希望执行任意的数学运算，而不是简单地依赖预定义的神经网 络层。

​		到目前为止，我们网络中的所有操作都对网络的激活值及网络的参数起作用。然而，有时我们可能希望合并 既不是上一层的结果也不是可更新参数的项，我们称之为常数参数（constant parameter）。例如，我们需要 一个计算函数 f(x, w) = c · w⊤x的层，其中x是输入，w是参数，c是某个在优化过程中没有更新的指定常量。 因此我们实现了一个FixedHiddenMLP类，如下所示：

```python
# 在前向传播中执行代码

class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数。因此其在训练期间保持不变
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # 使用创建的常量参数以及relu和mm函数
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # 复用全连接层。这相当于两个全连接层共享参数
        X = self.linear(X)
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
    
   
net = FixedHiddenMLP()
print(net)
net(X)
```

```cmd
FixedHiddenMLP(
  (linear): Linear(in_features=20, out_features=20, bias=True)
)
```

混合搭配使用：

```python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X))


X = torch.rand(2, 20)
print(X)
chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())

chimera(X)
```

```cmd
tensor([[0.1702, 0.9968, 0.5916, 0.6342, 0.6493, 0.6924, 0.8487, 0.1119, 0.4247,
         0.3286, 0.1218, 0.1853, 0.6032, 0.2335, 0.1116, 0.7278, 0.9940, 0.1214,
         0.5540, 0.3560],
        [0.5281, 0.7644, 0.4132, 0.7702, 0.4850, 0.0852, 0.8058, 0.1983, 0.4243,
         0.7049, 0.7050, 0.0394, 0.7400, 0.0279, 0.6208, 0.6732, 0.0214, 0.8269,
         0.3805, 0.1371]])
```



我们可以通过可视化工具来查看各个层级之间的输入输出:

```python
from torchinfo import summary

# print(X.size(), X.shape)
summary(chimera,input_data=X)
```

```cmd
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Sequential                               --                        --
├─NestMLP: 1-1                           [2, 16]                   --
│    └─Sequential: 2-1                   [2, 32]                   --
│    │    └─Linear: 3-1                  [2, 64]                   1,344
│    │    └─ReLU: 3-2                    [2, 64]                   --
│    │    └─Linear: 3-3                  [2, 32]                   2,080
│    │    └─ReLU: 3-4                    [2, 32]                   --
│    └─Linear: 2-2                       [2, 16]                   528
├─Linear: 1-2                            [2, 20]                   340
├─FixedHiddenMLP: 1-3                    --                        --
│    └─Linear: 2-3                       [2, 20]                   420
│    └─Linear: 2-4                       [2, 20]                   (recursive)
==========================================================================================
Total params: 4,712
Trainable params: 4,712
Non-trainable params: 0
Total mult-adds (M): 0.01
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.02
Estimated Total Size (MB): 0.02
==========================================================================================
```



## 2 参数管理

 		在选择了架构并设置了超参数后，我们就进入了训练阶段。此时，我们的目标是找到使损失函数最小化的模型参数值。经过训练后，我们将需要使用这些参数来做出未来的预测。此外，有时我们希望提取参数，以便在其他环境中复用它们，将模型保存下来，以便它可以在其他软件中执行，或者为了获得科学的理解而进行检查。 之前的介绍中，我们只依靠深度学习框架来完成训练的工作，而忽略了操作参数的具体细节。接下来我们将介绍以下内容： 

​	• 访问参数，用于调试、诊断和可视化；

​	• 参数初始化； 

​	• 在不同模型组件间共享参数。

 首先我们先定义一个具有单层隐藏层的多层感知机

```python
# 具有单隐藏层的多层感知机
import torch
from torch import nn

net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
X = torch.rand(size=(2, 4))
net(X)
```

### 2.1参数访问

​		我们从已有模型中访问参数。当通过Sequential类定义模型时，我们可以通过索引来访问模型的任意层。这 就像模型是一个列表一样，每层的参数都在其属性中。如下所示，我们可以检查第二个全连接层的参数。

```python
print(net[2].state_dict())
```

```cmd
OrderedDict([('weight', tensor([[ 0.2566, -0.2937,  0.0754,  0.2846, -0.1666, -0.2348, -0.3314,  0.1223]])), ('bias', tensor([-0.2706]))])
```

​		根据输出结果我们可以知道这个全连接参数包含两个参数，分别是该层的权重和偏置，两者都存储为单精度浮点值（float32）,注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。

#### 目标参数

​		每个参数都表示为参数类的一个实例，要对参数执行任何操作，我们都需要访问底层的数值。有几种方法可以做到这一点，有些比较简单，而另一些则比较通用。下面的代码从第二个全连接层（即第三个神 经网络层）提取偏置，提取后返回的是一个参数类实例，并进一步访问该参数的值。

```python
# 参数访问
print(type(net[0].bias))
print(net[2].bias)
print(net[2].bias.data)
net[2].weight.grad == None
```

```cmd
<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([0.0120], requires_grad=True)
tensor([0.0120])
True
```

​		参数是复合的对象，包含值，梯度和额外信息，这就是我们需要显式参数值的原因。除了值之外，我们还可 以访问每个参数的梯度。在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。

#### 一次性访问所有参数

​		我们也可以一次性访问所有参数，当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。当我们处理更复杂的块（例如，嵌套块） 时，情况可能会变得特别复杂，因为我们需要递归整个树来提取每个子块的参数。下面，我们将通过演示来 比较访问第一个全连接层的参数和访问所有层。

```python
net_0 = [(name, param) for name, param in net[0].named_parameters()]
net = [(name, param) for name, param in net.named_parameters()]
print(net_0)
print(net)

```



#### 从嵌套块收集参数

如果我们将多个块相互嵌套，参数命名约定该如何工作，我们首先定义一个生成块的函数（块工厂），然后将这些块组合到更大的块中。

```python
# 从嵌套块收集参数

def block1():
    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                         nn.Linear(8, 4), nn.ReLU())


def block2():
    net = nn.Sequential()
    for i in range(4):
        net.add_module(f"block {i}", block1())
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1))
X = torch.rand(size=(2, 4))
rgnet(X)
```

```cmd
Sequential(
  (0): Sequential(
    (block 0): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 1): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 2): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 3): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
  )
  (1): Linear(in_features=4, out_features=1, bias=True)
)
```

​		因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。下面，我们访问第一个主要的块 中、第二个子块的第一层的偏置项。

```python
rgnet[0][1][0].bias.data
```

```cmd
tensor([-0.3950, -0.1186,  0.4204,  0.2239, -0.4995, -0.0895,  0.2646, -0.1577])
```



### 2.2 参数初始化

​		深度学习框架提供默认随机初始化，也允许我们创建自定义初始化方法，满足我们通过其他规则实现初始化权重。 默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵，这个范围是根据输入和输出维度计算 出的。PyTorch的nn.init模块提供了多种预置初始化方法。

#### 内置初始化

下面的代码初始化分为两函数：

​		第一段函数将权重参数初始化为标准差为0.01的高斯随机变量，且将偏置参数设置为0。第二段函数将所有参数初始化为给定的常数，比如初始化为1。

```python
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU())

# 内置初始化
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)

net.apply(init_normal)
print(net[0].weight.data)
print(net[0].bias.data)

def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 1)
        nn.init.zeros_(m.bias)
net.apply(init_constant)

print(net[0].weight.data)
print(net[0].bias.data)



```

```cmd
tensor([[-0.0125,  0.0164,  0.0022, -0.0093],
        [-0.0200,  0.0093,  0.0118,  0.0055],
        [-0.0006,  0.0020,  0.0032,  0.0107],
        [ 0.0073, -0.0013, -0.0018, -0.0171],
        [-0.0037, -0.0148,  0.0049, -0.0067],
        [-0.0068,  0.0009,  0.0101,  0.0127],
        [ 0.0083, -0.0054,  0.0002,  0.0151],
        [-0.0075,  0.0050,  0.0070,  0.0024]])
tensor([0., 0., 0., 0., 0., 0., 0., 0.])
tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
tensor([0., 0., 0., 0., 0., 0., 0., 0.])
```

​		我们还可以对某些块应用不同的初始化方法。例如，下面我们使用Xavier初始化方法初始化第一个神经网络 层，然后将第三个神经网络层初始化为常量值42。

```python
def init_xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_normal_(m.weight)


def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)


net[0].apply(init_xavier)
net[2].apply(init_42)

print(net[0].weight.data)
print(net[2].weight.data)
```

```cmd
tensor([[ 0.5906, -0.3761, -0.0746, -0.1524],
        [ 0.0019,  0.5222, -0.2574, -0.1206],
        [-0.2953, -0.1453, -0.5212,  0.2671],
        [ 0.4837, -0.2348,  0.5743, -0.7950],
        [ 0.2432, -0.3427,  0.4302,  0.5886],
        [-0.5566, -0.0926, -0.6082,  0.1319],
        [ 0.1680,  1.1049, -0.0123, -0.2661],
        [-0.2075, -0.0806,  0.2600,  0.4267]])
tensor([[42., 42., 42., 42., 42., 42., 42., 42.],
        [42., 42., 42., 42., 42., 42., 42., 42.],
        [42., 42., 42., 42., 42., 42., 42., 42.],
        [42., 42., 42., 42., 42., 42., 42., 42.]])
```



#### 自定义初始化

有时候 ，深度学习框架没有提供我们所需要的初始化方法，在下面的例子中，我们使用以下的分布为任意权重参 数w定义初始化方法：

![image-20240520144823874](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240520144823874.png)

```python
def my_init(m):
    if type(m) == nn.Linear:
        print("Init：", *[(name, params.shape) for name, params in m.named_parameters()][0])
        nn.init.uniform_(m.weight, -10, 10)
        m.weight.data *= m.weight.data.abs() >= 5


net.apply(my_init)
print(net[0].weight[:2])
```

```cmd
Init： weight torch.Size([8, 4])
Init： weight torch.Size([4, 8])
tensor([[-5.0767, -0.0000,  5.2415,  0.0000],
        [ 0.0000,  0.0000, -7.4667, -6.2779]], grad_fn=<SliceBackward0>)
```

注意，我们始终可以直接设置参数

```python
# 直接设置参数
net[0].weight.data[:] += 1
net[0].weight.data[0, 0] = 42
print(net[0].weight.data[0])
```

```cmd
tensor([42.0000,  1.0000,  6.2415,  1.0000])
```



### 2.3 参数绑定

​		有时我们希望在多个层间共享参数：我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。

```python

shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))

X = torch.randn(2, 4)

net(X)

# 检查参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0, 0] = 100
# 确保它们实际上是同一个对象，而不只是有相同的值
print(net[2].weight.data[0] == net[4].weight.data[0])

```

```cmd
tensor([True, True, True, True, True, True, True, True])
tensor([True, True, True, True, True, True, True, True])
```

​		这个例子表明第三个和第五个神经网络层的参数是绑定的。它们不仅值相等，而且由相同的张量表示。因此如果我们改变其中一个参数，另一个参数也会改变。这里有一个问题：当参数绑定时，梯度会发生什么情况？ 答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层（即第三个神经网络层）和第三个隐藏层 （即第五个神经网络层）的梯度会加在一起。





## 3. 延后初始化

​		到目前为止，我们忽略了建立网络时需要做的以下这些事情：

​		• 我们定义了网络架构，但没有指定输入维度。 

​		• 我们添加层时没有指定前一层的输出维度。 

​		• 我们在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。 

​		有些读者可能会对我们的代码能运行感到惊讶。毕竟，深度学习框架无法判断网络的输入维度是什么。这里 的诀窍是框架的延后初始化（defers initialization），即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。 

​		在以后，当使用卷积神经网络时，由于输入维度（即图像的分辨率）将影响每个后续层的维数，有了该技术 将更加方便。现在我们在编写代码时无须知道维度是什么就可以设置参数，这种能力可以大大简化定义和修 改模型的任务。接下来，我们将更深入地研究初始化机制。



#### 3.1 实例化网络

​		具体步骤如下

```python
# 实例化网络

def init_xavier(m):
    if type(m) == nn.Linear:
        # 初始化所有隐藏层的权重参数为正态分布
        nn.init.normal_(m.weight, mean=0, std=1)


class my_net(nn.Module):
    def __init__(self):
        super(my_net, self).__init__()
        self.fc1 = nn.Linear(20, 16)
        self.fc2 = nn.Linear(16, 8)
        self.fc3 = nn.Linear(8, 1)

    def forward(self, x):
        x_1 = F.relu(self.fc1(x))
        x_2 = F.relu(self.fc2(x_1))
        x_3 = self.fc3(x_2)
        return x_3


net = nn.Sequential(nn.Linear(20, 16), nn.ReLU(),
                    nn.Linear(16, 8), nn.ReLU(),
                    nn.Linear(8, 1), nn.ReLU())
my_net = my_net()
print(my_net)
print(net)
net.apply(init_xavier)
X = torch.randn(4, 20)
my_net.apply(init_xavier)
print(net(X))
print("--------")
print(my_net(X))
```

```cmd
my_net(
  (fc1): Linear(in_features=20, out_features=16, bias=True)
  (fc2): Linear(in_features=16, out_features=8, bias=True)
  (fc3): Linear(in_features=8, out_features=1, bias=True)
)
Sequential(
  (0): Linear(in_features=20, out_features=16, bias=True)
  (1): ReLU()
  (2): Linear(in_features=16, out_features=8, bias=True)
  (3): ReLU()
  (4): Linear(in_features=8, out_features=1, bias=True)
  (5): ReLU()
)
tensor([[ 0.0000],
        [17.5516],
        [ 3.6322],
        [ 6.2948]], grad_fn=<ReluBackward0>)
--------
tensor([[ 6.5873],
        [ 5.7454],
        [10.8285],
        [11.8054]], grad_fn=<AddmmBackward0>)
```



​		首先，让我们实例化一个多层感知机。 此时，因为输入维数是未知的，所以网络不可能知道输入层权重的维数。因此，框架尚未初始化任何参数，我 们通过尝试访问以下参数进行确认。 接下来让我们将数据通过网络，最终使框架初始化参数。 

​		一旦我们知道输入维数是20，框架可以通过代入值20来识别第一层权重矩阵的形状。识别出第一层的形状后， 框架处理第二层，依此类推，直到所有形状都已知为止。注意，在这种情况下，只有第一层需要延迟初始化， 但是框架仍是按顺序初始化的。等到知道了所有的参数形状，框架就可以初始化参数。



## 4. 自定义层



​		深度学习成功背后的一个因素是神经网络的灵活性：我们可以用创造性的方式组合不同的层，从而设计出适 用于各种任务的架构。例如，研究人员发明了专门用于处理图像、文本、序列数据和执行动态规划的层。有时 我们会遇到或要自己发明一个现在在深度学习框架中还不存在的层。在这些情况下，必须构建自定义层。

#### 4.1 不带参数的层

​		首先，我们构造一个没有任何参数的自定义层。回忆一下在第一节对块的介绍，这应该看起来很眼熟。下面 的CenteredLayer类要从其输入中减去均值。要构建它，我们只需继承基础层类并实现前向传播功能。

```python
# 不带参数的层
class CanteredLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return x - torch.mean(x)
        # return x - x.mean()


layer = CanteredLayer()
# 创建一个浮点型tensor
tensor_x = torch.FloatTensor([1, 2, 3, 4, 5])
layer(tensor_x)
```

```cmd
tensor([-2., -1.,  0.,  1.,  2.])
```

将他嵌入其他模型进行计算

```python
net = nn.Sequential(nn.Linear(8, 128), nn.ReLU(), layer)
Y = net(torch.rand(4, 8))
print(torch.mean(Y))
```

```cmd
print(torch.mean(Y))
```



#### 4.2 带参数的层

​		下面我们继续定义具有参数的层，这些参数可以通过训练进行调整。我们可以使用内置函数来创建参数，这些函数提供一些基本的管理功能。比如管理访问、初始化、共享、保存 和加载模型参数。这样做的好处之一是：我们不需要为每个自定义层编写自定义的序列化程序。 

​		现在，让我们实现自定义版本的全连接层。回想一下，该层需要两个参数，一个用于表示权重，另一个用于 表示偏置项。在此实现中，我们使用修正线性单元作为激活函数。该层需要输入参数：in_units和units，分别表示输入数和输出数。

```python
# 带参数的层
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        """
        :param in_units: 输入数
        :param units: 输出数
        """
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units, ))

    def forward(self, x):
        linear = torch.matmul(x, self.weight.data) + self.bias.data
        return F.relu(linear)

net = MyLinear(8, 4)
print(net.weight.shape)
X = torch.rand(4, 8)
net(X)

```

```cmd
Parameter containing:
tensor([[ 0.6914,  0.9288, -0.3937, -0.9968],
        [-1.0557, -0.9531, -0.3243,  0.6061],
        [ 0.4608, -0.1198, -0.6561, -1.1528],
        [ 0.1746,  1.5458, -1.4646,  1.0698],
        [ 1.1977, -0.6775, -0.2000,  0.1253],
        [ 0.2986,  0.7157, -0.8244, -1.7264],
        [ 0.8367, -0.4479, -0.4334, -1.2894],
        [-0.4007,  1.8461,  0.7996, -0.1736]], requires_grad=True)
        
tensor([[1.8749, 3.0496, 0.0000, 0.0000],
        [1.0373, 2.4073, 0.0000, 0.0000],
        [1.2004, 1.9549, 0.0000, 0.0000],
        [0.9903, 2.9631, 0.0000, 0.0000]])
```

集合到复杂层集中

```python
net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))
net(torch.rand(4, 64))
```

```cmd
tensor([[ 8.9218],
        [ 3.2746],
        [10.0518],
        [ 2.6293]])
```





## 5. 读写文件

​		到目前为止，我们讨论了如何处理数据，以及如何构建、训练和测试深度学习模型。然而，有时我们希望保 存训练的模型，以备将来在各种环境中使用（比如在部署中进行预测）。此外，当运行一个耗时较长的训练过 程时，最佳的做法是定期保存中间结果，以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结 果。因此，现在是时候学习如何加载和存储权重向量和整个模型了。

### 5.1 加载和保存张量

```python
# 加载和保存张量

x = torch.rand(4, 5)
y = torch.rand(4)
print(x.shape, y.shape)
torch.save(x, 'x-file')
torch.save([x, y], 'x-y-file')
x_2 = torch.load('x-file')
print(x_2)
x_3, y_3 = torch.load("x-y-file")
print(x_3, y_3)
```

```cmd
torch.Size([4, 5]) torch.Size([4])
tensor([[0.9585, 0.4860, 0.1274, 0.3076, 0.8215],
        [0.1892, 0.5804, 0.7225, 0.4468, 0.6210],
        [0.9539, 0.3281, 0.9415, 0.3133, 0.3261],
        [0.0925, 0.5113, 0.2315, 0.7132, 0.8386]])
tensor([[0.9585, 0.4860, 0.1274, 0.3076, 0.8215],
        [0.1892, 0.5804, 0.7225, 0.4468, 0.6210],
        [0.9539, 0.3281, 0.9415, 0.3133, 0.3261],
        [0.0925, 0.5113, 0.2315, 0.7132, 0.8386]]) tensor([0.6522, 0.3408, 0.5701, 0.0670])
```



### 5.2 加载和保存模型参数

​		保存单个权重向量（或其他张量）确实有用，但是如果我们想保存整个模型，并在以后加载它们，单独保存 每个向量则会变得很麻烦。毕竟，我们可能有数百个参数散布在各处。因此，深度学习框架提供了内置函数 来保存和加载整个网络。需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型。例如，如 果我们有一个3层多层感知机，我们需要单独指定架构。因为模型本身可以包含任意代码，所以模型本身难以 序列化。因此，为了恢复模型，我们需要用代码生成架构，然后从磁盘加载参数。让我们从熟悉的多层感知 机开始尝试一下。

```python
# 加载和保存模型参数


class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)

    def forward(self, x):
        x = F.relu(self.hidden(x))
        return self.output(x)


net = MLP()
X = torch.rand(4, 20)
Y = net(X)

# ----------保存模型参数------------

torch.save(net.state_dict(), 'model/mlp.params')

```

在上面的代码示例中，我们首先定义了一个名叫MLP的模型，随后通过前向传播改变模型中的权重参数，最后将参数保存在`model/mlp.params`文件中。

在下面我们将重新实例化MLP模型，并从`model/mlp.params`加载参数

```python
clone = MLP()
clone.load_state_dict(torch.load('model/mlp.params'))
clone.eval()
```

```cmd
MLP(
  (hidden): Linear(in_features=20, out_features=256, bias=True)
  (output): Linear(in_features=256, out_features=10, bias=True)
)
```

由于两个实例具有相同的模型参数，在输入相同的X时，两个实例的计算结果应该相同。让我们来验证一下。

```python
Y_clone = clone(X)
print(Y_clone == Y)
```

```cmd
tensor([[True, True, True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True]])
```



## 6. GPU

​		本节，我们将讨论如何利用这种计算性能进行研究。首先是如何使用单个GPU，然后是如何使用多个GPU和 多个服务器（具有多个GPU）。 我们先看看如何使用单个NVIDIA GPU进行计算。首先，确保至少安装了一个NVIDIA GPU。然后，下 载NVIDIA驱动和CUDA并按照提示设置适当的路径。当这些准备工作完成，就可以使用nvidia-smi命令 来查看显卡信息。

```python
# 查看显卡信息
!nvidia-smi
```

​		在PyTorch中，每个数组都有一个设备（device），我们通常将其称为环境（context）。默认情况下，所有 变量和相关的计算都分配给CPU。有时环境可能是GPU。当我们跨多个服务器部署作业时，事情会变得更加 棘手。通过智能地将数组分配给环境，我们可以最大限度地减少在设备之间传输数据的时间。例如，当在带 有GPU的服务器上训练神经网络时，我们通常希望模型的参数在GPU上。 要运行此部分中的程序，至少需要两个GPU。



### 6.1 计算设备

​		我们可以指定用于存储和计算的设备，如CPU和GPU。默认情况下，张量是在内存中创建的，然后使用CPU计算它。 在PyTorch中，CPU和GPU可以用torch.device('cpu') 和torch.device('cuda')表示。应该注意的是，cpu设备意味着所有物理CPU和内存，这意味着PyTorch的计算将尝试使用所有CPU核心。然而，gpu设备只代表一 个卡和相应的显存。如果有多个GPU，我们使用torch.device(f'cuda:{i}') 来表示第i块GPU（i从0开始）。 另外，cuda:0和cuda是等价的。

```python
import torch
from torch import nn
torch.device('cpu'), torch.device('cuda'), torch.device('cuda:0')
```

```cmd
(device(type='cpu'), device(type='cuda'), device(type='cuda', index=0))
```

查询gpu数量

```python
# 查看gpu数量
torch.cuda.device_count()
```

```cmd
1
```

定义两个函数一个函数用于返回指定gpu如果gpu存在，另一个返回所有可用的gpu列表

```python
def try_gpu(i=0):
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')

def try_all_gpu():
    devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device('cpu')]

print(try_gpu())
print(try_gpu(10))
print(try_all_gpu())
```

```cmd
cuda:0
cpu
[device(type='cuda', index=0)]
```



### 6.2 张量与GPU

```python
# 默认创建的张量保存在cpu
x = torch.tensor([1, 2, 3])
print(x.device)
```

```cmd
cpu
```

​		需要注意的是，无论何时我们要对多个项进行操作，它们都必须在同一个设备上。例如，如果我们对两个张 量求和，我们需要确保两个张量都位于同一个设备上，否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。

对于在gpu上面存储张量，我们具有几种方式：

- 创建张量的时候指定设备
- 将在cpu创建的张量复制到gpu设备上面

![image-20240520171251384](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240520171251384.png)

```python
cpu_x = torch.rand(2, 3, requires_grad=True)
#创建张量的时候指定设备
gpu_y = torch.rand(2, 3, requires_grad=True, device=try_gpu())
#将在cpu创建的张量复制到gpu设备上面
gpu_x = cpu_x.cuda()
print(cpu_x)
print(gpu_x)
print(gpu_y)
```

```cmd
tensor([[0.9359, 0.1800, 0.5518],
        [0.6118, 0.3609, 0.3699]], requires_grad=True)
tensor([[0.9359, 0.1800, 0.5518],
        [0.6118, 0.3609, 0.3699]], device='cuda:0', grad_fn=<ToCopyBackward0>)
tensor([[0.3363, 0.6127, 0.1158],
        [0.7919, 0.5425, 0.1442]], device='cuda:0', requires_grad=True)
```

​		为什么使用GPU来进行机器学习，是因为单个GPU相对运行速度快。但是在设备（CPU、GPU和其他机器）之间 传输数据比计算慢得多。这也使得并行化变得更加困难，因为我们必须等待数据被发送（或者接收），然后 才能继续进行更多的操作。这就是为什么拷贝操作要格外小心。根据经验，多个小操作比一个大操作糟糕得 多。此外，一次执行几个操作比代码中散布的许多单个操作要好得多。如果一个设备必须等待另一个设备才 能执行其他操作，那么这样的操作可能会阻塞。这有点像排队订购咖啡，而不像通过电话预先订购：当客人 到店的时候，咖啡已经准备好了。 最后，当我们打印张量或将张量转换为NumPy格式时，如果数据不在内存中，框架会首先将其复制到内存中， 这会导致额外的传输开销。更糟糕的是，它现在受制于全局解释器锁，使得一切都得等待Python完成。



### 6.3 神经网络与GPU

类似地，神经网络模型可以指定设备。下面的代码将模型参数放在GPU上。

```python
# 神经网络与GPU
net = nn.Sequential(nn.Linear(3, 1))
net.to(device=try_gpu())

X = torch.rand(10, 3).to(device=try_gpu())

print(net(X))
print(net[0].weight.data)
print(net[0].bias.data.device)
```

```cmd
tensor([[-0.0177],
        [-0.1781],
        [-0.2571],
        [ 0.0722],
        [-0.3771],
        [-0.3206],
        [-0.3845],
        [ 0.0381],
        [-0.0109],
        [-0.3621]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.3831, -0.2679, -0.2493]], device='cuda:0')
cuda:0
```

只要所有的数据和参数都在同一个设备上，我们就可以有效地学习模型。

