# 现代卷积神经网络

​		在LeNet提出后，卷积神经网络在计算机视觉和机器学习领域中很有名气。但卷积神经网络并没有主导这些 领域。这是因为虽然LeNet在小数据集上取得了很好的效果，但是在更大、更真实的数据集上训练卷积神经 网络的性能和可行性还有待研究。事实上，在上世纪90年代初到2012年之间的大部分时间里，神经网络往往 被其他机器学习方法超越，如支持向量机（support vector machines）。

​		在计算机视觉中，直接将神经网络与其他机器学习方法进行比较也许不公平。这是因为，卷积神经网络的输 入是由原始像素值或是经过简单预处理（例如居中、缩放）的像素值组成的。但在使用传统机器学习方法时， 从业者永远不会将原始像素作为输入。在传统机器学习方法中，计算机视觉流水线是由经过人的手工精心设 计的特征流水线组成的。对于这些传统方法，大部分的进展都来自于对特征有了更聪明的想法，并且学习到 的算法往往归于事后的解释。

​		虽然上世纪90年代就有了一些神经网络加速卡，但仅靠它们还不足以开发出有大量参数的深层多通道多层卷 积神经网络。此外，当时的数据集仍然相对较小。除了这些障碍，训练神经网络的一些关键技巧仍然缺失，包 括启发式参数初始化、随机梯度下降的变体、非挤压激活函数和有效的正则化技术。 因此，与训练端到端（从像素到分类结果）系统不同，经典机器学习的流水线看起来更像下面这样： 

  		1. 获取一个有趣的数据集。在早期，收集这些数据集需要昂贵的传感器（在当时最先进的图像也就100万 像素）。 
  		2. 根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。 
  		3. 通过标准的特征提取算法，如SIFT（尺度不变特征变换）(Lowe, 2004)和SURF（加速鲁棒特征）(Bay et al., 2006)或其他手动调整的流水线来输入数据。 
  		4. 将提取的特征送入最喜欢的分类器中（例如线性模型或其它核方法），以训练分类器。

​		当人们和机器学习研究人员交谈时，会发现机器学习研究人员相信机器学习既重要又美丽：优雅的理论去证 明各种模型的性质。机器学习是一个正在蓬勃发展、严谨且非常有用的领域。然而，当人们和计算机视觉研 究人员交谈，会听到一个完全不同的故事。计算机视觉研究人员会告诉一个诡异事实————推动领域进步的 是数据特征，而不是学习算法。计算机视觉研究人员相信，从对最终模型精度的影响来说，更大或更干净的 数据集、或是稍微改进的特征提取，比任何学习算法带来的进步要大得多。

## 1. 深度卷积神经网络（AlexNet）

